This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.dockerignore
.env.example
.gitignore
Dockerfile
LICENSE
pyproject.toml
README.md
src/__init__.py
src/config.py
src/logging_config.py
src/main.py
src/middleware.py
src/utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".dockerignore">
# Fichiers Git
.git
.gitignore

# Environnements virtuels
.venv
venv
env
ENV

# Fichiers de configuration locaux
.env
*.env

# Cache Python
__pycache__/
*.pyc
*.pyo

# Fichiers Docker
Dockerfile
</file>

<file path=".env.example">
# --- Server Configuration ---
# The transport for the MCP server. Use 'http' for a web server or 'stdio'.
# This script currently runs HTTP by default.
TRANSPORT=http
MCP_HOST=0.0.0.0
MCP_PORT=8000
MCP_API_PATH=/mcp
# Secret key for signing Bearer tokens. If not set, server runs without authentication.
MCP_SERVER_SECRET_KEY=

# --- API Configuration ---
# The name of the OpenAPI specification file.
OPENAPI_URL=https://api.data.inclusion.beta.gouv.fr/api/openapi.json
# The name for the MCP server.
MCP_SERVER_NAME=DataInclusionAPI
# API key for the data.inclusion API.
DATA_INCLUSION_API_KEY=
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Environments
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
.venv/
.env


# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# VSCode
.vscode/

# Jupyter Notebook
.ipynb_checkpoints
</file>

<file path="Dockerfile">
# √âtape 1: Choisir une image de base
# On utilise une image Python 3.12 "slim" pour qu'elle soit l√©g√®re mais compl√®te.
FROM python:3.12-slim

# √âtape 2: D√©finir les arguments de build
# Permet de configurer le port au moment du build si n√©cessaire.
ARG PORT=8000

# √âtape 3: D√©finir le r√©pertoire de travail dans le conteneur
# C'est ici que les fichiers de notre application seront copi√©s.
WORKDIR /app

# √âtape 4: Installer un gestionnaire de paquets plus rapide (optionnel mais recommand√©)
# uv est une alternative tr√®s rapide √† pip.
RUN pip install uv

# √âtape 5: Copier les fichiers du projet
# On copie d'abord le fichier des d√©pendances pour profiter du cache de Docker.
COPY pyproject.toml ./

# √âtape 6: Installer les d√©pendances
# uv va lire pyproject.toml et installer les paquets list√©s.
# L'option --system installe les paquets globalement dans l'environnement Python du conteneur.
RUN uv pip install --system -r pyproject.toml

# √âtape 7: Copier le reste du code de l'application
COPY src/ ./src/

# √âtape 8: Exposer le port sur lequel l'application va √©couter
# Le conteneur rendra ce port disponible pour √™tre mapp√© sur l'h√¥te.
EXPOSE ${PORT}

# √âtape 9: D√©finir les variables d'environnement par d√©faut
# MCP_HOST est mis √† 0.0.0.0 pour √™tre accessible depuis l'ext√©rieur du conteneur.
# Les autres valeurs sont reprises de votre env.example.
# La cl√© API (DATA_INCLUSION_API_KEY) sera fournie au lancement, pas ici.
ENV TRANSPORT=http
ENV MCP_HOST=0.0.0.0
ENV MCP_PORT=${PORT}
ENV MCP_API_PATH=/mcp
ENV OPENAPI_URL=https://api.data.inclusion.beta.gouv.fr/api/openapi.json
ENV MCP_SERVER_NAME=DataInclusionAPI

# √âtape 10: D√©finir la commande pour lancer le serveur
# C'est la commande qui sera ex√©cut√©e au d√©marrage du conteneur.
CMD ["python", "-m", "src.main"]
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 andrew assef

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="pyproject.toml">
[project]
name = "datainclusion-mcp-server"
version = "0.1.0"
description = "A FastMCP server for the data.inclusion.beta.gouv.fr API."
readme = "README.md"
requires-python = ">=3.12"
license = { file = "LICENSE" }
authors = [
  { name = "Votre Nom", email = "votre@email.com" },
]
dependencies = [
    "fastmcp",
    "httpx",
    "python-dotenv",
    "langgraph",
    "langchain",
    "langchain-anthropic",
    "pydantic-settings"
]

[project.urls]
Homepage = "https://github.com/votre-user/datainclusion-mcp-server"

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"
</file>

<file path="README.md">
# Data Inclusion MCP Server

Un serveur MCP (Model Context Protocol) qui expose l'API [data.inclusion.beta.gouv.fr](https://data.inclusion.beta.gouv.fr) pour faciliter l'acc√®s aux donn√©es d'inclusion en France via des assistants IA compatibles MCP.

[![Licence: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

## üìã Description

Ce projet transforme automatiquement l'API REST de `data.inclusion` en outils MCP, permettant aux assistants IA (comme Claude Desktop) d'interroger facilement les donn√©es sur les structures, services et ressources d'inclusion sociale en France. Il charge la sp√©cification OpenAPI de l'API √† la vol√©e pour g√©n√©rer les outils.

### ‚ú® Fonctionnalit√©s

-   **üîÑ Conversion Automatique** : Transforme les endpoints de l'API en outils MCP √† la vol√©e.
-   **üîß Outils Conviviaux** : Noms d'outils renomm√©s pour une meilleure compr√©hension par les IA.
-   **üê≥ Support Docker** : Pr√™t √† l'emploi avec une configuration Docker simple.
-   **üîë Authentification S√©curis√©e** : G√®re l'authentification par `Bearer Token` via les variables d'environnement.
-   **‚öôÔ∏è Pagination Intelligente** : Limite automatiquement le nombre de r√©sultats pour des r√©ponses plus rapides et cibl√©es.

### üõ†Ô∏è Outils Disponibles

Le serveur expose plus d'une dizaine d'outils, dont les principaux :

-   `list_all_structures` : Liste les structures d'inclusion.
-   `get_structure_details` : Obtient les d√©tails d'une structure sp√©cifique.
-   `search_services` : Recherche des services selon des crit√®res (code postal, th√©matique, etc.).
-   `list_all_services` : Liste l'ensemble des services disponibles.
-   `doc_list_*` : Acc√®de aux diff√©rents r√©f√©rentiels (th√©matiques, types de frais, etc.).

## üöÄ D√©marrage Rapide avec Docker (Recommand√©)

Le moyen le plus simple de lancer le serveur est d'utiliser Docker.

### Pr√©requis

-   **Docker**
-   **Git**

### √âtapes

1.  **Cloner le repository :**
    ```bash
    git clone https://github.com/votre-user/datainclusion-mcp-server.git
    cd datainclusion-mcp-server
    ```

2.  **Configurer l'environnement :**
    -   Copiez le fichier d'exemple : `cp env.example .env`
    -   Ouvrez le fichier `.env` et ajoutez votre cl√© API : `DATA_INCLUSION_API_KEY=votre_cle_api_ici`
    -   **Important :** Laissez `MCP_HOST=0.0.0.0` pour que le conteneur soit accessible depuis votre machine.

3.  **Construire l'image Docker :**
    ```bash
    docker build -t datainclusion-mcp .
    ```

4.  **Lancer le conteneur :**
    ```bash
    docker run -d --rm -p 8000:8000 --env-file .env --name mcp-server datainclusion-mcp
    ```

5.  **V√©rifier les logs :**
    ```bash
    docker logs mcp-server
    ```
    Vous devriez voir `Starting MCP server on http://0.0.0.0:8000/mcp`. Votre serveur est pr√™t !

## üîå Int√©gration Client MCP (Claude Desktop, etc.)

Une fois le serveur lanc√© (localement ou via Docker), ajoutez cette configuration √† votre client MCP :

```json
{
  "mcpServers": {
    "data-inclusion": {
      "transport": "http",
      "url": "http://127.0.0.1:8000/mcp"
    }
  }
}
```

> **Localisation du fichier de config Claude :**
> - **Windows** : `%APPDATA%\Claude\claude_desktop_config.json`
> - **macOS** : `~/Library/Application Support/Claude/claude_desktop_config.json`
> - **Linux** : `~/.config/Claude/claude_desktop_config.json`

## ‚öôÔ∏è Installation et Lancement Manuels

Si vous ne souhaitez pas utiliser Docker.

### Pr√©requis

-   **Python 3.12+**

### √âtapes

1.  **Cloner le repository et naviguer dans le dossier.**
2.  **Installer les d√©pendances :**
    ```bash
    # Avec uv (recommand√©)
    uv pip install -e .
    
    # Ou avec pip
    pip install -e .
    ```
3.  **Configurer l'environnement :**
    -   `cp env.example .env`
    -   Ouvrez `.env` et ajoutez votre cl√© API.
    -   Pour un lancement local, `MCP_HOST=127.0.0.1` est suffisant.
4.  **Lancer le serveur :**
    ```bash
    python src/main.py
    ```

## üõ†Ô∏è Configuration des Variables d'Environnement

Configurez ces variables dans votre fichier `.env` :

| Variable                 | Description                                                               | D√©faut                                                    |
| ------------------------ | ------------------------------------------------------------------------- | --------------------------------------------------------- |
| `MCP_HOST`               | Adresse IP d'√©coute. **Utiliser `0.0.0.0` pour Docker.**                   | `127.0.0.1`                                               |
| `MCP_PORT`               | Port d'√©coute du serveur.                                                 | `8000`                                                    |
| `MCP_API_PATH`           | Chemin de l'endpoint de l'API MCP.                                        | `/mcp`                                                    |
| `OPENAPI_URL`            | URL de la sp√©cification OpenAPI √† charger.                                | `https://api.data.inclusion.beta.gouv.fr/api/openapi.json` |
| `MCP_SERVER_NAME`        | Nom du serveur affich√© dans les clients.                                  | `DataInclusionAPI`                                        |
| `DATA_INCLUSION_API_KEY` | **(Requis)** Votre cl√© API pour l'API `data.inclusion`.                   | **(Obligatoire)**                                         |

## üèóÔ∏è Structure du Projet

```
datainclusion-mcp-server/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Point d'entr√©e principal du serveur
‚îÇ   ‚îî‚îÄ‚îÄ utils.py             # Fonctions utilitaires (client HTTP, inspection)
‚îú‚îÄ‚îÄ .env.example             # Template de configuration d'environnement
‚îú‚îÄ‚îÄ .gitignore               # Fichiers ignor√©s par Git
‚îú‚îÄ‚îÄ Dockerfile               # Instructions pour construire l'image Docker
‚îú‚îÄ‚îÄ pyproject.toml           # D√©pendances et m√©tadonn√©es du projet
‚îî‚îÄ‚îÄ README.md                # Cette documentation
```

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une *Pull Request* ou une *Issue*.

1.  Forker le projet.
2.  Cr√©er une branche pour votre fonctionnalit√© (`git checkout -b feature/ma-super-feature`).
3.  Commiter vos changements (`git commit -m 'Ajout de ma-super-feature'`).
4.  Pousser vers la branche (`git push origin feature/ma-super-feature`).
5.  Ouvrir une Pull Request.

## üìù Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.
</file>

<file path="src/__init__.py">
# Package src
</file>

<file path="src/config.py">
"""
Configuration management using Pydantic Settings.

Ce module centralise toute la configuration de l'application en utilisant
Pydantic Settings pour une gestion robuste et typ√©e des variables d'environnement.
"""

from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """
    Configuration de l'application bas√©e sur Pydantic Settings.
    
    Cette classe charge automatiquement les variables d'environnement
    depuis le fichier .env et valide leur type.
    """
    
    # Configuration de l'API OpenAPI
    OPENAPI_URL: str = "https://api.data.inclusion.beta.gouv.fr/api/openapi.json"
    
    # Configuration du serveur MCP
    MCP_SERVER_NAME: str = "DataInclusionAPI"
    MCP_HOST: str = "127.0.0.1"
    MCP_PORT: int = 8000
    MCP_API_PATH: str = "/mcp"
    
    # Cl√©s d'API et authentification
    DATA_INCLUSION_API_KEY: str
    MCP_SERVER_SECRET_KEY: str | None = None
    
    # Configuration Pydantic Settings
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore"  # Ignore les variables d'environnement non d√©finies
    )
</file>

<file path="src/logging_config.py">
"""
Configuration du syst√®me de journalisation pour le serveur MCP.
"""

import logging
import sys
from typing import Optional


def setup_logging(level: str = "INFO") -> logging.Logger:
    """
    Configure un logger standard pour le serveur MCP.
    
    Args:
        level: Niveau de logging (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        
    Returns:
        logging.Logger: Instance du logger configur√©
    """
    # Convertir le niveau string en constante logging
    numeric_level = getattr(logging, level.upper(), logging.INFO)
    
    # Cr√©er le logger principal
    logger = logging.getLogger("MCP_SERVER")
    logger.setLevel(numeric_level)
    
    # √âviter la duplication des handlers si d√©j√† configur√©
    if logger.handlers:
        return logger
    
    # Cr√©er un formatter avec timestamp, niveau et message
    formatter = logging.Formatter(
        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Cr√©er un handler pour la console
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(numeric_level)
    console_handler.setFormatter(formatter)
    
    # Ajouter le handler au logger
    logger.addHandler(console_handler)
    
    return logger
</file>

<file path="src/main.py">
"""
DataInclusion MCP Server

Ce serveur MCP expose l'API data.inclusion.beta.gouv.fr via le protocole Model Context Protocol.
Il transforme automatiquement les endpoints OpenAPI en outils MCP.
"""

import asyncio
import json
import logging
import os
import httpx
from fastmcp import FastMCP
from fastmcp.tools import Tool
from fastmcp.tools.tool_transform import ArgTransform
from fastmcp.server.openapi import RouteMap, MCPType
from fastmcp.server.auth import BearerAuthProvider
from fastmcp.server.auth.providers.bearer import RSAKeyPair
from fastmcp.utilities.components import FastMCPComponent
from fastmcp.utilities.openapi import parse_openapi_to_http_routes, HTTPRoute

from .config import Settings
from .utils import inspect_mcp_components, create_api_client, deep_clean_schema, find_route_by_id
from .logging_config import setup_logging
from .middleware import ErrorHandlingMiddleware, TimingMiddleware

def limit_page_size_in_spec(spec: dict, logger: logging.Logger, max_size: int = 25) -> dict:
    """
    Modifie la sp√©cification OpenAPI pour limiter la taille des pages.

    Cette fonction parcourt les points de terminaison pertinents et ajuste le param√®tre
    'size' pour qu'il ait une valeur maximale et par d√©faut de `max_size`.

    Args:
        spec: Le dictionnaire de la sp√©cification OpenAPI.
        logger: Instance du logger pour les messages.
        max_size: La taille maximale √† d√©finir pour les r√©sultats.

    Returns:
        Le dictionnaire de la sp√©cification modifi√©.
    """
    paths_to_modify = [
        "/api/v0/structures",
        "/api/v0/services",
        "/api/v0/search/services",
    ]

    logger.info(f"Applying page size limit (max_size={max_size}) to spec...")

    for path in paths_to_modify:
        if path in spec["paths"] and "get" in spec["paths"][path]:
            params = spec["paths"][path]["get"].get("parameters", [])
            for param in params:
                if param.get("name") == "size":
                    param["schema"]["maximum"] = max_size
                    param["schema"]["default"] = max_size
                    logger.info(f"  - Limited 'size' parameter for endpoint: GET {path}")
    
    return spec


def customize_for_gemini(route, component, logger: logging.Logger):
    """
    Simplifie les sch√©mas d'un composant pour une meilleure compatibilit√©
    avec les mod√®les stricts comme Gemini, en retirant les titres.
    """
    tool_name = getattr(component, 'name', 'Unknown')
    cleaned_schemas = []
    
    # Nettoyer le sch√©ma d'entr√©e
    if hasattr(component, 'input_schema') and component.input_schema:
        deep_clean_schema(component.input_schema)
        cleaned_schemas.append("input schema")
        logger.info(f"Input schema cleaned for tool: {tool_name}")
    
    # Nettoyer le sch√©ma de sortie
    if hasattr(component, 'output_schema') and component.output_schema:
        deep_clean_schema(component.output_schema)
        cleaned_schemas.append("output schema")
        logger.info(f"Output schema cleaned for tool: {tool_name}")
    
    # Message de r√©sum√© si des sch√©mas ont √©t√© nettoy√©s
    if cleaned_schemas:
        logger.info(f"Schema cleaning completed for tool '{tool_name}': {', '.join(cleaned_schemas)}")
    else:
        logger.debug(f"No schemas found to clean for tool: {tool_name}")


def discover_and_customize(route: HTTPRoute, component: FastMCPComponent, logger: logging.Logger, op_id_map: dict[str, str]):
    """
    Personnalise le composant pour Gemini et d√©couvre le nom de l'outil g√©n√©r√©.
    """
    # Appel de la fonction de personnalisation existante
    customize_for_gemini(route, component, logger)
    
    # D√©couverte du nom de l'outil et stockage dans la map
    if hasattr(route, 'operation_id') and route.operation_id and hasattr(component, 'name') and component.name:
        op_id_map[route.operation_id] = component.name


async def main():
    """
    Fonction principale qui configure et lance le serveur MCP.
    
    Cette fonction :
    1. Charge la configuration
    2. Charge la sp√©cification OpenAPI
    3. Cr√©e un client HTTP authentifi√©
    4. Configure le serveur MCP avec des noms d'outils personnalis√©s
    5. Lance le serveur avec le transport SSE
    """
    
    # === 0. CHARGEMENT DE LA CONFIGURATION ===
    settings = Settings()
    
    # Dictionnaire pour stocker la correspondance entre operation_id et noms d'outils g√©n√©r√©s
    op_id_to_mangled_name: dict[str, str] = {}
    
    # === 1. CONFIGURATION DU LOGGING ===
    logger = setup_logging()
    
    api_client = None
    
    try:
        # === 2. CHARGEMENT DE LA SP√âCIFICATION OPENAPI VIA HTTP ===
        logger.info(f"Loading OpenAPI specification from URL: '{settings.OPENAPI_URL}'...")
        
        try:
            # On a besoin d'importer httpx si ce n'est pas d√©j√† fait en haut du fichier
            
            async with httpx.AsyncClient() as client:
                response = await client.get(settings.OPENAPI_URL)
                response.raise_for_status()  # L√®ve une exception si le statut n'est pas 2xx
                openapi_spec = response.json()
            
            api_title = openapi_spec.get("info", {}).get("title", "Unknown API")
            logger.info(f"Successfully loaded OpenAPI spec: '{api_title}'")
            
            # === PR√â-PARSING DE LA SP√âCIFICATION OPENAPI ===
            logger.info("Parsing OpenAPI specification to HTTP routes...")
            http_routes = parse_openapi_to_http_routes(openapi_spec)
            logger.info(f"Successfully parsed {len(http_routes)} HTTP routes from OpenAPI specification")
            
            # === MODIFICATION DES LIMITES DE PAGINATION ===
            # Limite la taille des pages pour les outils de listing √† 25 √©l√©ments maximum
            # Cela s'applique aux outils: list_all_structures, list_all_services, search_services
            logger.info("Applying pagination limits to data-listing endpoints...")
            openapi_spec = limit_page_size_in_spec(openapi_spec, logger=logger, max_size=25)
            
        except httpx.RequestError as e:
            logger.error(f"Failed to fetch OpenAPI specification from '{settings.OPENAPI_URL}'.")
            logger.error(f"Details: {e}")
            return
            
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in the response from '{settings.OPENAPI_URL}'.")
            logger.error(f"Details: {e}")
            return

        # === 3. D√âTERMINATION DE L'URL DE BASE ===
        servers = openapi_spec.get("servers", [])
        if servers and isinstance(servers, list) and len(servers) > 0 and "url" in servers[0]:
            base_url = servers[0]["url"]
            logger.info(f"Using base URL from OpenAPI spec: {base_url}")
        else:
            base_url = "http://localhost:8000"
            logger.warning("No servers section found in OpenAPI spec.")
            logger.warning(f"Using default base URL: {base_url}")

        # === 4. CR√âATION DU CLIENT HTTP AUTHENTIFI√â ===
        logger.info("Configuring HTTP client with authentication...")
        api_client = create_api_client(base_url, logger, settings.DATA_INCLUSION_API_KEY)

        # === 5. CONFIGURATION DES NOMS D'OUTILS PERSONNALIS√âS ===
        logger.info("Configuring custom tool names...")
        
        # Mapping des noms d'op√©rations OpenAPI vers des noms d'outils MCP plus conviviaux
        # Note: Noms courts pour respecter la limite de 60 caract√®res de FastMCP
        custom_mcp_tool_names = {
            # Endpoints de Structures
            "list_structures_endpoint_api_v0_structures_get": "list_all_structures",
            "retrieve_structure_endpoint_api_v0_structures__source___id__get": "get_structure_details",

            # Endpoints de Sources
            "list_sources_endpoint_api_v0_sources_get": "list_all_sources",

            # Endpoints de Services
            "list_services_endpoint_api_v0_services_get": "list_all_services",
            "retrieve_service_endpoint_api_v0_services__source___id__get": "get_service_details",
            "search_services_endpoint_api_v0_search_services_get": "search_services",

            # Endpoints de Documentation
            "as_dict_list_api_v0_doc_labels_nationaux_get": "doc_list_labels_nationaux",
            "as_dict_list_api_v0_doc_thematiques_get": "doc_list_thematiques",
            "as_dict_list_api_v0_doc_typologies_services_get": "doc_list_typologies_services",
            "as_dict_list_api_v0_doc_frais_get": "doc_list_frais",
            "as_dict_list_api_v0_doc_profils_get": "doc_list_profils_publics",
            "as_dict_list_api_v0_doc_typologies_structures_get": "doc_list_typologies_structures",
            "as_dict_list_api_v0_doc_modes_accueil_get": "doc_list_modes_accueil",
            
            # Endpoints modes_orientation (NOMS RACCOURCIS pour respecter limite 60 caract√®res)
            "as_dict_list_api_v0_doc_modes_orientation_accompagnateur_get": "doc_modes_orient_accomp",
            "as_dict_list_api_v0_doc_modes_orientation_beneficiaire_get": "doc_modes_orient_benef",
        }

        # === 6. CONFIGURATION DES ROUTES MCP ===
        logger.info("Configuring route mappings...")
        
        # Configuration pour mapper tous les endpoints GET comme des outils MCP
        custom_route_maps = [
            RouteMap(methods=["GET"], pattern=r".*", mcp_type=MCPType.TOOL),
        ]

        # === 7. CONFIGURATION DE L'AUTHENTIFICATION ===
        logger.info("Configuring server authentication...")
        
        # Lecture de la cl√© secr√®te depuis la configuration
        secret_key = settings.MCP_SERVER_SECRET_KEY
        auth_provider = None
        
        if secret_key and secret_key.strip():
            logger.info("Secret key found - configuring Bearer Token authentication...")
            try:
                # Si la cl√© ressemble √† une cl√© RSA priv√©e PEM, l'utiliser directement
                if secret_key.strip().startswith("-----BEGIN") and "PRIVATE KEY" in secret_key:
                    # Utiliser la cl√© priv√©e pour cr√©er une paire de cl√©s
                    from cryptography.hazmat.primitives import serialization
                    private_key = serialization.load_pem_private_key(
                        secret_key.encode(), password=None
                    )
                    public_key_pem = private_key.public_key().public_bytes(
                        encoding=serialization.Encoding.PEM,
                        format=serialization.PublicFormat.SubjectPublicKeyInfo
                    ).decode()
                    
                    auth_provider = BearerAuthProvider(
                        public_key=public_key_pem,
                        audience="datainclusion-mcp-client"
                    )
                else:
                    # Utiliser la cl√© comme seed pour g√©n√©rer une paire de cl√©s d√©terministe
                    # Pour des raisons de simplicit√©, on g√©n√®re une nouvelle paire de cl√©s
                    key_pair = RSAKeyPair.generate()
                    
                    auth_provider = BearerAuthProvider(
                        public_key=key_pair.public_key,
                        audience="datainclusion-mcp-client"
                    )
                    
                    # Log du token de test (UNIQUEMENT pour le d√©veloppement)
                    test_token = key_pair.create_token(
                        audience="datainclusion-mcp-client",
                        subject="test-user",
                        expires_in_seconds=3600
                    )
                    logger.info(f"üîë Test Bearer Token (for development): {test_token}")
                
                logger.info("‚úì Bearer Token authentication configured successfully")
                logger.info("   - Audience: datainclusion-mcp-client")
                logger.info("   - Server will require valid Bearer tokens for access")
                
            except Exception as e:
                logger.error(f"Failed to configure authentication: {e}")
                logger.warning("Continuing without authentication...")
                auth_provider = None
        else:
            logger.warning("MCP_SERVER_SECRET_KEY not set - server will run WITHOUT authentication")
            logger.warning("‚ö†Ô∏è  All clients will have unrestricted access to the server")

        # === 8. CR√âATION DU SERVEUR MCP ===
        logger.info(f"Creating FastMCP server '{settings.MCP_SERVER_NAME}'...")
        
        mcp_server = FastMCP.from_openapi(
            openapi_spec=openapi_spec,
            client=api_client,
            name=settings.MCP_SERVER_NAME,
            route_maps=custom_route_maps,
            auth=auth_provider,
            mcp_component_fn=lambda route, component: discover_and_customize(route, component, logger, op_id_to_mangled_name)
        )
        
        logger.info(f"FastMCP server '{mcp_server.name}' created successfully!")
        logger.info("   - Custom GET-to-Tool mapping applied")

        # === 9. AJOUT DES MIDDLEWARES ===
        logger.info("Adding middleware stack...")
        
        # Ajouter le middleware de gestion d'erreurs EN PREMIER
        # Il doit capturer toutes les erreurs des autres middlewares
        error_handling_middleware = ErrorHandlingMiddleware(logger)
        mcp_server.add_middleware(error_handling_middleware)
        logger.info("   - Error handling middleware added successfully")
        
        # Ajouter le middleware de timing APR√àS la gestion d'erreurs
        timing_middleware = TimingMiddleware(logger)
        mcp_server.add_middleware(timing_middleware)
        logger.info("   - Timing middleware added successfully")

        # === 10. RENOMMAGE ET ENRICHISSEMENT AVANC√â DES OUTILS ===
        logger.info("Applying advanced tool transformations using Tool.from_tool()...")
        
        successful_renames = 0
        total_tools = len(custom_mcp_tool_names)
        
        for original_name, new_name in custom_mcp_tool_names.items():
            # Rechercher la route correspondante dans les donn√©es OpenAPI
            route = await find_route_by_id(original_name, http_routes)
            if route is None:
                logger.warning(f"  ‚úó Route not found for operation_id: '{original_name}' - skipping transformation")
                continue
            
            # Utilise la map pour obtenir le nom de l'outil g√©n√©r√© par FastMCP
            mangled_tool_name = op_id_to_mangled_name.get(original_name)
            if not mangled_tool_name:
                logger.warning(f"  ‚úó Could not find a generated tool for operation_id: '{original_name}' - skipping transformation")
                continue
            
            try:
                # R√©cup√©rer l'outil original en utilisant son nom "mangl√©"
                original_tool = await mcp_server.get_tool(mangled_tool_name)
                if not original_tool:
                    logger.warning(f"  ‚úó Tool not found: '{mangled_tool_name}' (may have been renamed during OpenAPI processing)")
                    continue
                
                # === ENRICHISSEMENT DES ARGUMENTS ===
                arg_transforms = {}
                param_count = 0
                
                # Enrichir les descriptions des param√®tres depuis l'OpenAPI
                if hasattr(route, 'parameters') and route.parameters:
                    for param in route.parameters:
                        if hasattr(param, 'name') and param.name:
                            transforms = {}
                            
                            # Ajouter une description si disponible
                            if hasattr(param, 'description') and param.description and param.description.strip():
                                transforms['description'] = param.description.strip()
                                param_count += 1
                            
                            # Ajouter des exemples si disponibles
                            if hasattr(param, 'example') and param.example:
                                transforms['examples'] = [param.example]
                            
                            # Cr√©er l'ArgTransform seulement s'il y a des transformations
                            if transforms:
                                arg_transforms[param.name] = ArgTransform(**transforms)
                                logger.debug(f"    - Enriching parameter '{param.name}': {list(transforms.keys())}")
                
                # === CR√âATION DE LA DESCRIPTION ENRICHIE ===
                tool_description = None
                if hasattr(route, 'description') and route.description and route.description.strip():
                    tool_description = route.description.strip()
                elif hasattr(route, 'summary') and route.summary and route.summary.strip():
                    # Fallback vers le summary si pas de description
                    tool_description = route.summary.strip()
                else:
                    # Description par d√©faut bas√©e sur le nom de l'outil
                    tool_description = f"Execute the {new_name} operation on the Data Inclusion API"
                
                # === AJOUT DE TAGS POUR ORGANISATION ===
                tool_tags = {"data-inclusion", "api"}
                
                # Ajouter des tags sp√©cifiques selon le type d'endpoint
                if "list_all" in new_name or "search" in new_name:
                    tool_tags.add("listing")
                if "get_" in new_name and "details" in new_name:
                    tool_tags.add("details")
                if "doc_" in new_name:
                    tool_tags.add("documentation")
                if any(endpoint in new_name for endpoint in ["structures", "services", "sources"]):
                    tool_tags.add("core-data")
                
                # === CR√âATION DU NOUVEL OUTIL TRANSFORM√â ===
                transformed_tool = Tool.from_tool(
                    tool=original_tool,
                    name=new_name,
                    description=tool_description,
                    transform_args=arg_transforms if arg_transforms else None,
                    tags=tool_tags
                )
                
                # === AJOUT ET SUPPRESSION ===
                # Ajouter le nouvel outil au serveur
                mcp_server.add_tool(transformed_tool)
                
                # IMPORTANT: Supprimer l'outil original pour √©viter les doublons
                # et la confusion pour le LLM
                try:
                    mcp_server.remove_tool(mangled_tool_name)
                    logger.debug(f"    - Removed original tool: '{mangled_tool_name}'")
                except Exception as remove_error:
                    # En cas d'√©chec de suppression, d√©sactiver au moins l'outil
                    logger.debug(f"    - Could not remove '{mangled_tool_name}', disabling instead: {remove_error}")
                    original_tool.disable()
                
                # === LOGGING DE SUCC√àS ===
                successful_renames += 1
                enrichment_info = []
                
                if tool_description:
                    enrichment_info.append("description")
                if param_count > 0:
                    enrichment_info.append(f"{param_count} param descriptions")
                if tool_tags:
                    enrichment_info.append(f"{len(tool_tags)} tags")
                
                enrichment_msg = f" (enriched: {', '.join(enrichment_info)})" if enrichment_info else ""
                logger.info(f"  ‚úì Transformed tool: '{original_name}' -> '{new_name}'{enrichment_msg}")
                
            except Exception as e:
                logger.error(f"  ‚úó Failed to transform tool '{original_name}' -> '{new_name}': {e}")
                logger.debug(f"    Exception details: {type(e).__name__}: {str(e)}")
        
        # === R√âSUM√â FINAL ===
        if successful_renames > 0:
            logger.info(f"‚úì Tool transformation completed: {successful_renames}/{total_tools} tools successfully transformed")
        else:
            logger.warning(f"‚ö†Ô∏è  No tools were successfully transformed out of {total_tools} attempted")
        
        # V√©rifier que nous avons encore des outils apr√®s transformation
        final_tools = await mcp_server.get_tools()
        enabled_tools = [name for name, tool in final_tools.items() if tool.enabled]
        logger.info(f"üìä Final tool count: {len(enabled_tools)} enabled tools available")
        
        # === DEBUG: AFFICHER LES OPERATION_IDS DISPONIBLES ===
        # Afficher les operation_ids non mapp√©s pour aider au debug
        logger.info("=== OpenAPI Route Analysis ===")
        available_ops = [route.operation_id for route in http_routes if hasattr(route, 'operation_id') and route.operation_id]
        unmapped_ops = [op_id for op_id in available_ops if op_id not in custom_mcp_tool_names]
        
        logger.info(f"Total OpenAPI routes: {len(available_ops)}")
        logger.info(f"Mapped routes: {len(custom_mcp_tool_names)}")
        logger.info(f"Unmapped routes: {len(unmapped_ops)}")
        
        if unmapped_ops:
            logger.info("‚ö†Ô∏è  Unmapped operation_ids (should be added to custom_mcp_tool_names):")
            for op_id in sorted(unmapped_ops):
                logger.info(f"  - '{op_id}'")

        # === 11. INSPECTION DES COMPOSANTS MCP ===
        logger.info("Inspecting MCP components...")
        await inspect_mcp_components(mcp_server, logger)

        # === 12. LANCEMENT DU SERVEUR ===
        server_url = f"http://{settings.MCP_HOST}:{settings.MCP_PORT}{settings.MCP_API_PATH}"
        logger.info(f"Starting MCP server on {server_url}")
        logger.info("Press Ctrl+C to stop the server")
        
        await mcp_server.run_async(
            transport="http",
            host=settings.MCP_HOST,
            port=settings.MCP_PORT,
            path=settings.MCP_API_PATH
        )

    except KeyboardInterrupt:
        logger.info("Server stopped by user")
        
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        logger.error("Please check your configuration and try again.")
        
    finally:
        # === 13. NETTOYAGE DES RESSOURCES ===
        if api_client:
            logger.info("Closing HTTP client...")
            await api_client.aclose()
            logger.info("HTTP client closed successfully")


if __name__ == "__main__":
    """
    Point d'entr√©e du script.
    Lance le serveur MCP avec gestion d'erreurs appropri√©e.
    """
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nGoodbye!")
    except Exception as e:
        print(f"Failed to start server: {e}")
        exit(1)
</file>

<file path="src/middleware.py">
"""
Middleware personnalis√©s pour le serveur MCP DataInclusion.
"""

import time
import logging
import httpx
from fastmcp.server.middleware import Middleware, MiddlewareContext
from fastmcp.exceptions import ToolError, ResourceError
from mcp import McpError
from mcp.types import ErrorData


class ErrorHandlingMiddleware(Middleware):
    """
    Middleware pour capturer les exceptions et les transformer en erreurs MCP standardis√©es.
    
    Ce middleware intercepte toutes les exceptions non g√©r√©es et les transforme en
    erreurs MCP standardis√©es avec des codes d'erreur appropri√©s pour le client.
    
    Gestion sp√©cialis√©e pour :
    - McpError : Re-lev√©es sans modification (d√©j√† standardis√©es)
    - ToolError/ResourceError : Messages sp√©cifiques pr√©serv√©s 
    - HTTPStatusError : Extraction des d√©tails d'erreur API
    - Autres exceptions : Pr√©servation du message original quand informatif
    """
    
    def __init__(self, logger: logging.Logger):
        """
        Initialise le middleware de gestion d'erreurs.
        
        Args:
            logger: Instance du logger pour enregistrer les erreurs
        """
        self.logger = logger
    
    async def on_request(self, context: MiddlewareContext, call_next):
        """
        Intercepte les requ√™tes MCP pour capturer et standardiser les erreurs.
        
        Args:
            context: Contexte de la requ√™te MCP contenant les m√©tadonn√©es
            call_next: Fonction pour continuer la cha√Æne de middleware
            
        Returns:
            Le r√©sultat de la requ√™te apr√®s traitement
            
        Raises:
            McpError: Exception MCP standardis√©e avec ErrorData appropri√©
        """
        try:
            # Traiter la requ√™te normalement
            return await call_next(context)
            
        except McpError:
            # Re-lever les erreurs MCP d√©j√† standardis√©es sans modification
            raise
            
        except (ToolError, ResourceError) as e:
            # Logger l'erreur sp√©cifique des outils/ressources MCP
            self.logger.error(
                f"MCP tool/resource error in {context.method}: {type(e).__name__}: {e}",
                exc_info=True
            )
            
            # Cr√©er une ErrorData avec le message sp√©cifique de l'outil
            # Code -32603 : Erreur interne (selon la spec JSON-RPC)
            error_data = ErrorData(
                code=-32603,
                message=str(e)  # Pr√©server le message sp√©cifique de l'outil
            )
            
            # Lever une McpError standardis√©e pour le client
            raise McpError(error_data)
            
        except httpx.HTTPStatusError as e:
            # Logger l'erreur HTTP de mani√®re d√©taill√©e
            self.logger.error(
                f"HTTP status error in {context.method}: {e.response.status_code} {e.response.reason_phrase}",
                exc_info=True
            )
            
            # Essayer de parser la r√©ponse JSON pour extraire les d√©tails
            error_details = None
            try:
                error_details = e.response.json()
            except Exception:
                # Si on ne peut pas parser le JSON, utiliser le texte brut
                try:
                    error_details = {"message": e.response.text}
                except Exception:
                    error_details = {"message": "Unable to parse error response"}
            
            # Construire un message d'erreur clair
            status_code = e.response.status_code
            reason = e.response.reason_phrase or "Unknown Error"
            
            if isinstance(error_details, dict):
                # Extraire le message d'erreur principal
                api_message = (
                    error_details.get("error", {}).get("message") if isinstance(error_details.get("error"), dict)
                    else error_details.get("message") 
                    or error_details.get("detail")
                    or error_details.get("error")
                    or "No error details available"
                )
                
                detailed_message = f"API Error {status_code} ({reason}): {api_message}"
                
                # Ajouter des d√©tails suppl√©mentaires si disponibles
                if "error_code" in error_details:
                    detailed_message += f" [Code: {error_details['error_code']}]"
                elif isinstance(error_details.get("error"), dict) and "code" in error_details["error"]:
                    detailed_message += f" [Code: {error_details['error']['code']}]"
                    
            else:
                detailed_message = f"API Error {status_code} ({reason}): {str(error_details)}"
            
            # Cr√©er une ErrorData avec le code Invalid Params (-32602)
            error_data = ErrorData(
                code=-32602,
                message=detailed_message
            )
            
            # Lever une McpError standardis√©e pour le client
            raise McpError(error_data)
            
        except Exception as e:
            # Logger l'erreur de mani√®re d√©taill√©e
            self.logger.error(
                f"Unhandled exception in {context.method}: {type(e).__name__}: {e}",
                exc_info=True  # Inclut la stack trace compl√®te
            )
            
            # Pr√©server le message d'erreur original s'il est informatif
            original_message = str(e).strip()
            if original_message and len(original_message) < 500:  # √âviter les messages trop longs
                detailed_message = f"{type(e).__name__}: {original_message}"
            else:
                detailed_message = f"Internal server error: {type(e).__name__}"
            
            # Transformer l'exception en erreur MCP standardis√©e
            # Code -32000 : Erreur interne du serveur (selon la spec JSON-RPC)
            error_data = ErrorData(
                code=-32000,
                message=detailed_message
            )
            
            # Lever une McpError standardis√©e pour le client
            raise McpError(error_data)


class TimingMiddleware(Middleware):
    """
    Middleware pour mesurer et journaliser le temps d'ex√©cution des requ√™tes MCP.
    
    Ce middleware intercepte toutes les requ√™tes MCP, mesure leur temps d'ex√©cution
    et enregistre les m√©triques de performance via le syst√®me de logging.
    """
    
    def __init__(self, logger: logging.Logger):
        """
        Initialise le middleware de timing.
        
        Args:
            logger: Instance du logger pour enregistrer les m√©triques de performance
        """
        self.logger = logger
    
    async def on_request(self, context: MiddlewareContext, call_next):
        """
        Intercepte les requ√™tes MCP pour mesurer leur temps d'ex√©cution.
        
        Args:
            context: Contexte de la requ√™te MCP contenant les m√©tadonn√©es
            call_next: Fonction pour continuer la cha√Æne de middleware
            
        Returns:
            Le r√©sultat de la requ√™te apr√®s traitement
        """
        # Enregistrer le temps de d√©but
        start_time = time.perf_counter()
        
        try:
            # Traiter la requ√™te
            result = await call_next(context)
            
            # Calculer la dur√©e en millisecondes
            duration_ms = (time.perf_counter() - start_time) * 1000
            
            # Journaliser le succ√®s de la requ√™te
            self.logger.info(
                f"Request {context.method} completed in {duration_ms:.2f}ms"
            )
            
            return result
            
        except Exception as e:
            # Calculer la dur√©e m√™me en cas d'erreur
            duration_ms = (time.perf_counter() - start_time) * 1000
            
            # Journaliser l'√©chec de la requ√™te
            self.logger.warning(
                f"Request {context.method} failed after {duration_ms:.2f}ms: {type(e).__name__}: {e}"
            )
            
            # Re-lever l'exception pour ne pas interrompre le flux d'erreur
            raise
</file>

<file path="src/utils.py">
import logging
import httpx
from fastmcp import FastMCP
from fastmcp.utilities.openapi import HTTPRoute


async def inspect_mcp_components(mcp_instance: FastMCP, logger: logging.Logger):
    """Inspecte et affiche les composants MCP (outils, ressources, templates)."""
    logger.info("--- Inspecting MCP Components ---")
    tools = await mcp_instance.get_tools()
    resources = await mcp_instance.get_resources()
    templates = await mcp_instance.get_resource_templates()

    # S√©parer les outils activ√©s et d√©sactiv√©s
    enabled_tools = [t for t in tools.values() if t.enabled and t.name is not None]
    disabled_tools = [t for t in tools.values() if not t.enabled and t.name is not None]
    
    logger.info(f"{len(tools)} Total Tool(s) found ({len(enabled_tools)} enabled, {len(disabled_tools)} disabled):")
    if enabled_tools:
        logger.info(f"  Enabled Tools: {', '.join(sorted([t.name for t in enabled_tools]))}")
    else:
        logger.info("  No enabled tools found.")
    
    if disabled_tools:
        logger.debug(f"  Disabled Tools: {', '.join(sorted([t.name for t in disabled_tools]))}")

    logger.info(f"{len(resources)} Resource(s) found:")
    if resources:
        logger.info(f"  Names: {', '.join(sorted([r.name for r in resources.values() if r.name is not None]))}")
    else:
        logger.info("  No resources generated.")

    logger.info(f"{len(templates)} Resource Template(s) found:")
    if templates:
        logger.info(f"  Names: {', '.join(sorted([t.name for t in templates.values() if t.name is not None]))}")
    else:
        logger.info("  No resource templates generated.")
    logger.info("--- End of MCP Components Inspection ---")


def create_api_client(base_url: str, logger: logging.Logger, api_key: str | None = None) -> httpx.AsyncClient:
    """Cr√©e un client HTTP avec authentification pour l'API Data Inclusion.
    
    Args:
        base_url: L'URL de base de l'API
        logger: Instance du logger pour les messages
        api_key: Cl√© d'API optionnelle pour l'authentification
        
    Returns:
        httpx.AsyncClient: Client HTTP configur√© avec les headers d'authentification
    """
    headers = {}
    
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
        logger.info(f"Using DATA_INCLUSION_API_KEY from configuration (key: ***{api_key[-4:]})")
    else:
        logger.warning("DATA_INCLUSION_API_KEY not set in configuration.")
        logger.warning("Some API endpoints may be publicly accessible, but authenticated endpoints will fail.")
        logger.warning("Please set DATA_INCLUSION_API_KEY in your .env file if you have an API key.")
    
    # Ajout d'headers par d√©faut
    headers.update({
        "User-Agent": "DataInclusion-MCP-Server/1.0",
        "Accept": "application/json"
    })
    
    return httpx.AsyncClient(
        base_url=base_url, 
        headers=headers,
        timeout=30.0  # Timeout de 30 secondes
    )


def deep_clean_schema(schema: dict) -> None:
    """Nettoie r√©cursivement un sch√©ma JSON en supprimant tous les champs "title".
    
    Cette fonction parcourt r√©cursivement un dictionnaire repr√©sentant un sch√©ma JSON
    et supprime toutes les cl√©s "title" trouv√©es, y compris dans les dictionnaires 
    imbriqu√©s et les listes de dictionnaires.
    
    Args:
        schema: Dictionnaire repr√©sentant un sch√©ma JSON √† nettoyer
        
    Note:
        Cette fonction modifie le dictionnaire en place et ne retourne rien.
    """
    if not isinstance(schema, dict):
        return
    
    # Collecter les cl√©s "title" √† supprimer pour √©viter de modifier 
    # le dictionnaire pendant l'it√©ration
    keys_to_remove = []
    
    for key, value in schema.items():
        if key == "title":
            keys_to_remove.append(key)
        elif isinstance(value, dict):
            # Nettoyer r√©cursivement les dictionnaires imbriqu√©s
            deep_clean_schema(value)
        elif isinstance(value, list):
            # Nettoyer r√©cursivement les √©l√©ments de liste qui sont des dictionnaires
            for item in value:
                if isinstance(item, dict):
                    deep_clean_schema(item)
    
    # Supprimer toutes les cl√©s "title" collect√©es
    for key in keys_to_remove:
        del schema[key]


async def find_route_by_id(operation_id: str, routes: list[HTTPRoute]) -> HTTPRoute | None:
    """
    Recherche un objet HTTPRoute par son operation_id.
    
    Cette fonction parcourt une liste d'objets HTTPRoute et retourne le premier
    objet dont l'attribut operation_id correspond √† l'operation_id fourni.
    
    Args:
        operation_id: L'identifiant d'op√©ration √† rechercher
        routes: La liste des objets HTTPRoute √† parcourir
        
    Returns:
        HTTPRoute | None: L'objet HTTPRoute correspondant ou None si aucune correspondance n'est trouv√©e
    """
    for route in routes:
        if hasattr(route, 'operation_id') and route.operation_id == operation_id:
            return route
    return None
</file>

</files>

Tu es un architecte logiciel expert et un d√©veloppeur senior. Ton unique mission est de g√©n√©rer une suite de prompts pour un assistant de code IA (Cursor) afin d'impl√©menter la fonctionnalit√© d√©crite ci-dessous. Ne fournis pas d'extraits de code, g√©n√®re les instructions pour que l'autre IA le fasse.

---

## Contexte

- **Objectif Global :** ` Aide' moi √† r√©soudre le probl√®me de comaptibilit√© avec les mod√®les Gemini, j'ai trouve √ßa sur internet : 
Skip to main content

ai.google.dev uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more.
Google AI for Developers

Models
/
Sign in
Gemini API docs
API Reference
Cookbook
Community

Introducing updates to our 2.5 family of thinking models. Learn more

    Home
    Gemini API
    Models
    API Reference

Was this helpful?
Caching

Context caching allows you to save and reuse precomputed input tokens that you wish to use repeatedly, for example when asking different questions about the same media file. This can lead to cost and speed savings, depending on the usage. For a detailed introduction, see the Context caching guide.
Method: cachedContents.create

Creates CachedContent resource.
Endpoint
post https://generativelanguage.googleapis.com/v1beta/cachedContents
Request body

The request body contains an instance of CachedContent.
Fields
contents[] object (Content)

Optional. Input only. Immutable. The content to cache.
tools[] object (Tool)

Optional. Input only. Immutable. A list of Tools the model may use to generate the next response
expiration Union type
Specifies when this resource will expire. expiration can be only one of the following:
expireTime string (Timestamp format)

Timestamp in UTC of when this resource is considered expired. This is always provided on output, regardless of what was sent on input.

Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits. Offsets other than "Z" are also accepted. Examples: "2014-10-02T15:01:23Z", "2014-10-02T15:01:23.045123456Z" or "2014-10-02T15:01:23+05:30".
ttl string (Duration format)

Input only. New TTL for this resource, input only.

A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
displayName string

Optional. Immutable. The user-generated meaningful display name of the cached content. Maximum 128 Unicode characters.
model string

Required. Immutable. The name of the Model to use for cached content Format: models/{model}
systemInstruction object (Content)

Optional. Input only. Immutable. Developer set system instruction. Currently text only.
toolConfig object (ToolConfig)

Optional. Input only. Immutable. Tool config. This config is shared for all tools.
Example request
Basic
From name
From chat
Python
Node.js
Go
Shell

from google import genai
from google.genai import types

client = genai.Client()
document = client.files.upload(file=media / "a11.txt")
model_name = "gemini-1.5-flash-001"

cache = client.caches.create(
    model=model_name,
    config=types.CreateCachedContentConfig(
        contents=[document],
        system_instruction="You are an expert analyzing transcripts.",
    ),
)
print(cache)

response = client.models.generate_content(
    model=model_name,
    contents="Please summarize this transcript",
    config=types.GenerateContentConfig(cached_content=cache.name),
)
print(response.text)

Response body

If successful, the response body contains a newly created instance of CachedContent.
Method: cachedContents.list

Lists CachedContents.
Endpoint
get https://generativelanguage.googleapis.com/v1beta/cachedContents
Query parameters
pageSize integer

Optional. The maximum number of cached contents to return. The service may return fewer than this value. If unspecified, some default (under maximum) number of items will be returned. The maximum value is 1000; values above 1000 will be coerced to 1000.
pageToken string

Optional. A page token, received from a previous cachedContents.list call. Provide this to retrieve the subsequent page.

When paginating, all other parameters provided to cachedContents.list must match the call that provided the page token.
Request body

The request body must be empty.
Response body

Response with CachedContents list.

If successful, the response body contains data with the following structure:
Fields
cachedContents[] object (CachedContent)

List of cached contents.
nextPageToken string

A token, which can be sent as pageToken to retrieve the next page. If this field is omitted, there are no subsequent pages.
JSON representation

{
  "cachedContents": [
    {
      object (CachedContent)
    }
  ],
  "nextPageToken": string
}

Method: cachedContents.get

Reads CachedContent resource.
Endpoint
get https://generativelanguage.googleapis.com/v1beta/{name=cachedContents/*}
Path parameters
name string

Required. The resource name referring to the content cache entry. Format: cachedContents/{id} It takes the form cachedContents/{cachedcontent}.
Request body

The request body must be empty.
Example request
Python
Node.js
Go
Shell

from google import genai

client = genai.Client()
document = client.files.upload(file=media / "a11.txt")
model_name = "gemini-1.5-flash-001"

cache = client.caches.create(
    model=model_name,
    config={
        "contents": [document],
        "system_instruction": "You are an expert analyzing transcripts.",
    },
)
print(client.caches.get(name=cache.name))

Response body

If successful, the response body contains an instance of CachedContent.
Method: cachedContents.patch

Updates CachedContent resource (only expiration is updatable).
Endpoint
patch https://generativelanguage.googleapis.com/v1beta/{cachedContent.name=cachedContents/*}

PATCH https://generativelanguage.googleapis.com/v1beta/{cachedContent.name=cachedContents/*}
Path parameters
cachedContent.name string

Output only. Identifier. The resource name referring to the cached content. Format: cachedContents/{id} It takes the form cachedContents/{cachedcontent}.
Query parameters
updateMask string (FieldMask format)

The list of fields to update.

This is a comma-separated list of fully qualified names of fields. Example: "user.displayName,photo".
Request body

The request body contains an instance of CachedContent.
Fields
expiration Union type
Specifies when this resource will expire. expiration can be only one of the following:
expireTime string (Timestamp format)

Timestamp in UTC of when this resource is considered expired. This is always provided on output, regardless of what was sent on input.

Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits. Offsets other than "Z" are also accepted. Examples: "2014-10-02T15:01:23Z", "2014-10-02T15:01:23.045123456Z" or "2014-10-02T15:01:23+05:30".
ttl string (Duration format)

Input only. New TTL for this resource, input only.

A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
Example request
Python
Node.js
Go
Shell

from google import genai
from google.genai import types
import datetime

client = genai.Client()
document = client.files.upload(file=media / "a11.txt")
model_name = "gemini-1.5-flash-001"

cache = client.caches.create(
    model=model_name,
    config={
        "contents": [document],
        "system_instruction": "You are an expert analyzing transcripts.",
    },
)

# Update the cache's time-to-live (ttl)
ttl = f"{int(datetime.timedelta(hours=2).total_seconds())}s"
client.caches.update(
    name=cache.name, config=types.UpdateCachedContentConfig(ttl=ttl)
)
print(f"After update:\n {cache}")

# Alternatively, update the expire_time directly
# Update the expire_time directly in valid RFC 3339 format (UTC with a "Z" suffix)
expire_time = (
    (
        datetime.datetime.now(datetime.timezone.utc)
        + datetime.timedelta(minutes=15)
    )
    .isoformat()
    .replace("+00:00", "Z")
)
client.caches.update(
    name=cache.name,
    config=types.UpdateCachedContentConfig(expire_time=expire_time),
)

Response body

If successful, the response body contains an instance of CachedContent.
Method: cachedContents.delete

Deletes CachedContent resource.
Endpoint
delete https://generativelanguage.googleapis.com/v1beta/{name=cachedContents/*}
Path parameters
name string

Required. The resource name referring to the content cache entry Format: cachedContents/{id} It takes the form cachedContents/{cachedcontent}.
Request body

The request body must be empty.
Example request
Python
Node.js
Go
Shell

from google import genai

client = genai.Client()
document = client.files.upload(file=media / "a11.txt")
model_name = "gemini-1.5-flash-001"

cache = client.caches.create(
    model=model_name,
    config={
        "contents": [document],
        "system_instruction": "You are an expert analyzing transcripts.",
    },
)
client.caches.delete(name=cache.name)

Response body

If successful, the response body is an empty JSON object.
REST Resource: cachedContents
Resource: CachedContent

Content that has been preprocessed and can be used in subsequent request to GenerativeService.

Cached content can be only used with model it was created for.
Fields
contents[] object (Content)

Optional. Input only. Immutable. The content to cache.
tools[] object (Tool)

Optional. Input only. Immutable. A list of Tools the model may use to generate the next response
createTime string (Timestamp format)

Output only. Creation time of the cache entry.

Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits. Offsets other than "Z" are also accepted. Examples: "2014-10-02T15:01:23Z", "2014-10-02T15:01:23.045123456Z" or "2014-10-02T15:01:23+05:30".
updateTime string (Timestamp format)

Output only. When the cache entry was last updated in UTC time.

Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits. Offsets other than "Z" are also accepted. Examples: "2014-10-02T15:01:23Z", "2014-10-02T15:01:23.045123456Z" or "2014-10-02T15:01:23+05:30".
usageMetadata object (UsageMetadata)

Output only. Metadata on the usage of the cached content.
expiration Union type
Specifies when this resource will expire. expiration can be only one of the following:
expireTime string (Timestamp format)

Timestamp in UTC of when this resource is considered expired. This is always provided on output, regardless of what was sent on input.

Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits. Offsets other than "Z" are also accepted. Examples: "2014-10-02T15:01:23Z", "2014-10-02T15:01:23.045123456Z" or "2014-10-02T15:01:23+05:30".
ttl string (Duration format)

Input only. New TTL for this resource, input only.

A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
name string

Output only. Identifier. The resource name referring to the cached content. Format: cachedContents/{id}
displayName string

Optional. Immutable. The user-generated meaningful display name of the cached content. Maximum 128 Unicode characters.
model string

Required. Immutable. The name of the Model to use for cached content Format: models/{model}
systemInstruction object (Content)

Optional. Input only. Immutable. Developer set system instruction. Currently text only.
toolConfig object (ToolConfig)

Optional. Input only. Immutable. Tool config. This config is shared for all tools.
JSON representation

{
  "contents": [
    {
      object (Content)
    }
  ],
  "tools": [
    {
      object (Tool)
    }
  ],
  "createTime": string,
  "updateTime": string,
  "usageMetadata": {
    object (UsageMetadata)
  },

  // expiration
  "expireTime": string,
  "ttl": string
  // Union type
  "name": string,
  "displayName": string,
  "model": string,
  "systemInstruction": {
    object (Content)
  },
  "toolConfig": {
    object (ToolConfig)
  }
}

Content

The base structured datatype containing multi-part content of a message.

A Content includes a role field designating the producer of the Content and a parts field containing multi-part data that contains the content of the message turn.
Fields
parts[] object (Part)

Ordered Parts that constitute a single message. Parts may have different MIME types.
role string

Optional. The producer of the content. Must be either 'user' or 'model'.

Useful to set for multi-turn conversations, otherwise can be left blank or unset.
JSON representation

{
  "parts": [
    {
      object (Part)
    }
  ],
  "role": string
}

Part

A datatype containing media that is part of a multi-part Content message.

A Part consists of data which has an associated datatype. A Part can only contain one of the accepted types in Part.data.

A Part must have a fixed IANA MIME type identifying the type and subtype of the media if the inlineData field is filled with raw bytes.
Fields
thought boolean

Optional. Indicates if the part is thought from the model.
data Union type
data can be only one of the following:
text string

Inline text.
inlineData object (Blob)

Inline media bytes.
functionCall object (FunctionCall)

A predicted FunctionCall returned from the model that contains a string representing the FunctionDeclaration.name with the arguments and their values.
functionResponse object (FunctionResponse)

The result output of a FunctionCall that contains a string representing the FunctionDeclaration.name and a structured JSON object containing any output from the function is used as context to the model.
fileData object (FileData)

URI based data.
executableCode object (ExecutableCode)

Code generated by the model that is meant to be executed.
codeExecutionResult object (CodeExecutionResult)

Result of executing the ExecutableCode.
metadata Union type
Controls extra preprocessing of data. metadata can be only one of the following:
videoMetadata object (VideoMetadata)

Optional. Video metadata. The metadata should only be specified while the video data is presented in inlineData or fileData.
JSON representation

{
  "thought": boolean,

  // data
  "text": string,
  "inlineData": {
    object (Blob)
  },
  "functionCall": {
    object (FunctionCall)
  },
  "functionResponse": {
    object (FunctionResponse)
  },
  "fileData": {
    object (FileData)
  },
  "executableCode": {
    object (ExecutableCode)
  },
  "codeExecutionResult": {
    object (CodeExecutionResult)
  }
  // Union type

  // metadata
  "videoMetadata": {
    object (VideoMetadata)
  }
  // Union type
}

Blob

Raw media bytes.

Text should not be sent as raw bytes, use the 'text' field.
Fields
mimeType string

The IANA standard MIME type of the source data. Examples: - image/png - image/jpeg If an unsupported MIME type is provided, an error will be returned. For a complete list of supported types, see Supported file formats.
data string (bytes format)

Raw bytes for media formats.

A base64-encoded string.
JSON representation

{
  "mimeType": string,
  "data": string
}

FunctionCall

A predicted FunctionCall returned from the model that contains a string representing the FunctionDeclaration.name with the arguments and their values.
Fields
id string

Optional. The unique id of the function call. If populated, the client to execute the functionCall and return the response with the matching id.
name string

Required. The name of the function to call. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 63.
args object (Struct format)

Optional. The function parameters and values in JSON object format.
JSON representation

{
  "id": string,
  "name": string,
  "args": {
    object
  }
}

FunctionResponse

The result output from a FunctionCall that contains a string representing the FunctionDeclaration.name and a structured JSON object containing any output from the function is used as context to the model. This should contain the result of aFunctionCall made based on model prediction.
Fields
id string

Optional. The id of the function call this response is for. Populated by the client to match the corresponding function call id.
name string

Required. The name of the function to call. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 63.
response object (Struct format)

Required. The function response in JSON object format.
willContinue boolean

Optional. Signals that function call continues, and more responses will be returned, turning the function call into a generator. Is only applicable to NON_BLOCKING function calls, is ignored otherwise. If set to false, future responses will not be considered. It is allowed to return empty response with willContinue=False to signal that the function call is finished. This may still trigger the model generation. To avoid triggering the generation and finish the function call, additionally set scheduling to SILENT.
scheduling enum (Scheduling)

Optional. Specifies how the response should be scheduled in the conversation. Only applicable to NON_BLOCKING function calls, is ignored otherwise. Defaults to WHEN_IDLE.
JSON representation

{
  "id": string,
  "name": string,
  "response": {
    object
  },
  "willContinue": boolean,
  "scheduling": enum (Scheduling)
}

Scheduling

Specifies how the response should be scheduled in the conversation.
Enums
SCHEDULING_UNSPECIFIED 	This value is unused.
SILENT 	Only add the result to the conversation context, do not interrupt or trigger generation.
WHEN_IDLE 	Add the result to the conversation context, and prompt to generate output without interrupting ongoing generation.
INTERRUPT 	Add the result to the conversation context, interrupt ongoing generation and prompt to generate output.
FileData

URI based data.
Fields
mimeType string

Optional. The IANA standard MIME type of the source data.
fileUri string

Required. URI.
JSON representation

{
  "mimeType": string,
  "fileUri": string
}

ExecutableCode

Code generated by the model that is meant to be executed, and the result returned to the model.

Only generated when using the CodeExecution tool, in which the code will be automatically executed, and a corresponding CodeExecutionResult will also be generated.
Fields
language enum (Language)

Required. Programming language of the code.
code string

Required. The code to be executed.
JSON representation

{
  "language": enum (Language),
  "code": string
}

Language

Supported programming languages for the generated code.
Enums
LANGUAGE_UNSPECIFIED 	Unspecified language. This value should not be used.
PYTHON 	Python >= 3.10, with numpy and simpy available.
CodeExecutionResult

Result of executing the ExecutableCode.

Only generated when using the CodeExecution, and always follows a part containing the ExecutableCode.
Fields
outcome enum (Outcome)

Required. Outcome of the code execution.
output string

Optional. Contains stdout when code execution is successful, stderr or other description otherwise.
JSON representation

{
  "outcome": enum (Outcome),
  "output": string
}

Outcome

Enumeration of possible outcomes of the code execution.
Enums
OUTCOME_UNSPECIFIED 	Unspecified status. This value should not be used.
OUTCOME_OK 	Code execution completed successfully.
OUTCOME_FAILED 	Code execution finished but with a failure. stderr should contain the reason.
OUTCOME_DEADLINE_EXCEEDED 	Code execution ran for too long, and was cancelled. There may or may not be a partial output present.
VideoMetadata

Metadata describes the input video content.
Fields
startOffset string (Duration format)

Optional. The start offset of the video.

A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
endOffset string (Duration format)

Optional. The end offset of the video.

A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
fps number

Optional. The frame rate of the video sent to the model. If not specified, the default value will be 1.0. The fps range is (0.0, 24.0].
JSON representation

{
  "startOffset": string,
  "endOffset": string,
  "fps": number
}

Tool

Tool details that the model may use to generate response.

A Tool is a piece of code that enables the system to interact with external systems to perform an action, or set of actions, outside of knowledge and scope of the model.
Fields
functionDeclarations[] object (FunctionDeclaration)

Optional. A list of FunctionDeclarations available to the model that can be used for function calling.

The model or system does not execute the function. Instead the defined function may be returned as a FunctionCall with arguments to the client side for execution. The model may decide to call a subset of these functions by populating FunctionCall in the response. The next conversation turn may contain a FunctionResponse with the Content.role "function" generation context for the next model turn.
googleSearchRetrieval object (GoogleSearchRetrieval)

Optional. Retrieval tool that is powered by Google search.
codeExecution object (CodeExecution)

Optional. Enables the model to execute code as part of generation.
googleSearch object (GoogleSearch)

Optional. GoogleSearch tool type. Tool to support Google Search in Model. Powered by Google.
urlContext object (UrlContext)

Optional. Tool to support URL context retrieval.
JSON representation

{
  "functionDeclarations": [
    {
      object (FunctionDeclaration)
    }
  ],
  "googleSearchRetrieval": {
    object (GoogleSearchRetrieval)
  },
  "codeExecution": {
    object (CodeExecution)
  },
  "googleSearch": {
    object (GoogleSearch)
  },
  "urlContext": {
    object (UrlContext)
  }
}

FunctionDeclaration

Structured representation of a function declaration as defined by the OpenAPI 3.03 specification. Included in this declaration are the function name and parameters. This FunctionDeclaration is a representation of a block of code that can be used as a Tool by the model and executed by the client.
Fields
name string

Required. The name of the function. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 63.
description string

Required. A brief description of the function.
behavior enum (Behavior)

Optional. Specifies the function Behavior. Currently only supported by the BidiGenerateContent method.
parameters object (Schema)

Optional. Describes the parameters to this function. Reflects the Open API 3.03 Parameter Object string Key: the name of the parameter. Parameter names are case sensitive. Schema Value: the Schema defining the type used for the parameter.
response object (Schema)

Optional. Describes the output from this function in JSON Schema format. Reflects the Open API 3.03 Response Object. The Schema defines the type used for the response value of the function.
JSON representation

{
  "name": string,
  "description": string,
  "behavior": enum (Behavior),
  "parameters": {
    object (Schema)
  },
  "response": {
    object (Schema)
  }
}

Schema

The Schema object allows the definition of input and output data types. These types can be objects, but also primitives and arrays. Represents a select subset of an OpenAPI 3.0 schema object.
Fields
type enum (Type)

Required. Data type.
format string

Optional. The format of the data. This is used only for primitive datatypes. Supported formats: for NUMBER type: float, double for INTEGER type: int32, int64 for STRING type: enum, date-time
title string

Optional. The title of the schema.
description string

Optional. A brief description of the parameter. This could contain examples of use. Parameter description may be formatted as Markdown.
nullable boolean

Optional. Indicates if the value may be null.
enum[] string

Optional. Possible values of the element of Type.STRING with enum format. For example we can define an Enum Direction as : {type:STRING, format:enum, enum:["EAST", NORTH", "SOUTH", "WEST"]}
maxItems string (int64 format)

Optional. Maximum number of the elements for Type.ARRAY.
minItems string (int64 format)

Optional. Minimum number of the elements for Type.ARRAY.
properties map (key: string, value: object (Schema))

Optional. Properties of Type.OBJECT.

An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
required[] string

Optional. Required properties of Type.OBJECT.
minProperties string (int64 format)

Optional. Minimum number of the properties for Type.OBJECT.
maxProperties string (int64 format)

Optional. Maximum number of the properties for Type.OBJECT.
minLength string (int64 format)

Optional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING
maxLength string (int64 format)

Optional. Maximum length of the Type.STRING
pattern string

Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
example value (Value format)

Optional. Example of the object. Will only populated when the object is the root.
anyOf[] object (Schema)

Optional. The value should be validated against any (one or more) of the subschemas in the list.
propertyOrdering[] string

Optional. The order of the properties. Not a standard field in open api spec. Used to determine the order of the properties in the response.
default value (Value format)

Optional. Default value of the field. Per JSON Schema, this field is intended for documentation generators and doesn't affect validation. Thus it's included here and ignored so that developers who send schemas with a default field don't get unknown-field errors.
items object (Schema)

Optional. Schema of the elements of Type.ARRAY.
minimum number

Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER
maximum number

Optional. Maximum value of the Type.INTEGER and Type.NUMBER
JSON representation

{
  "type": enum (Type),
  "format": string,
  "title": string,
  "description": string,
  "nullable": boolean,
  "enum": [
    string
  ],
  "maxItems": string,
  "minItems": string,
  "properties": {
    string: {
      object (Schema)
    },
    ...
  },
  "required": [
    string
  ],
  "minProperties": string,
  "maxProperties": string,
  "minLength": string,
  "maxLength": string,
  "pattern": string,
  "example": value,
  "anyOf": [
    {
      object (Schema)
    }
  ],
  "propertyOrdering": [
    string
  ],
  "default": value,
  "items": {
    object (Schema)
  },
  "minimum": number,
  "maximum": number
}

Type

Type contains the list of OpenAPI data types as defined by https://spec.openapis.org/oas/v3.0.3#data-types
Enums
TYPE_UNSPECIFIED 	Not specified, should not be used.
STRING 	String type.
NUMBER 	Number type.
INTEGER 	Integer type.
BOOLEAN 	Boolean type.
ARRAY 	Array type.
OBJECT 	Object type.
NULL 	Null type.
Behavior

Defines the function behavior. Defaults to BLOCKING.
Enums
UNSPECIFIED 	This value is unused.
BLOCKING 	If set, the system will wait to receive the function response before continuing the conversation.
NON_BLOCKING 	If set, the system will not wait to receive the function response. Instead, it will attempt to handle function responses as they become available while maintaining the conversation between the user and the model.
GoogleSearchRetrieval

Tool to retrieve public web data for grounding, powered by Google.
Fields
dynamicRetrievalConfig object (DynamicRetrievalConfig)

Specifies the dynamic retrieval configuration for the given source.
JSON representation

{
  "dynamicRetrievalConfig": {
    object (DynamicRetrievalConfig)
  }
}

DynamicRetrievalConfig

Describes the options to customize dynamic retrieval.
Fields
mode enum (Mode)

The mode of the predictor to be used in dynamic retrieval.
dynamicThreshold number

The threshold to be used in dynamic retrieval. If not set, a system default value is used.
JSON representation

{
  "mode": enum (Mode),
  "dynamicThreshold": number
}

Mode

The mode of the predictor to be used in dynamic retrieval.
Enums
MODE_UNSPECIFIED 	Always trigger retrieval.
MODE_DYNAMIC 	Run retrieval only when system decides it is necessary.
CodeExecution

This type has no fields.

Tool that executes code generated by the model, and automatically returns the result to the model.

See also ExecutableCode and CodeExecutionResult which are only generated when using this tool.
GoogleSearch

GoogleSearch tool type. Tool to support Google Search in Model. Powered by Google.
Fields
timeRangeFilter object (Interval)

Optional. Filter search results to a specific time range. If customers set a start time, they must set an end time (and vice versa).
JSON representation

{
  "timeRangeFilter": {
    object (Interval)
  }
}

Interval

Represents a time interval, encoded as a Timestamp start (inclusive) and a Timestamp end (exclusive).

The start must be less than or equal to the end. When the start equals the end, the interval is empty (matches no time). When both start and end are unspecified, the interval matches any time.
Fields
startTime string (Timestamp format)

Optional. Inclusive start of the interval.

If specified, a Timestamp matching this interval will have to be the same or after the start.

Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits. Offsets other than "Z" are also accepted. Examples: "2014-10-02T15:01:23Z", "2014-10-02T15:01:23.045123456Z" or "2014-10-02T15:01:23+05:30".
endTime string (Timestamp format)

Optional. Exclusive end of the interval.

If specified, a Timestamp matching this interval will have to be before the end.

Uses RFC 3339, where generated output will always be Z-normalized and uses 0, 3, 6 or 9 fractional digits. Offsets other than "Z" are also accepted. Examples: "2014-10-02T15:01:23Z", "2014-10-02T15:01:23.045123456Z" or "2014-10-02T15:01:23+05:30".
JSON representation

{
  "startTime": string,
  "endTime": string
}

UrlContext

This type has no fields.

Tool to support URL context retrieval.
ToolConfig

The Tool configuration containing parameters for specifying Tool use in the request.
Fields
functionCallingConfig object (FunctionCallingConfig)

Optional. Function calling config.
JSON representation

{
  "functionCallingConfig": {
    object (FunctionCallingConfig)
  }
}

FunctionCallingConfig

Configuration for specifying function calling behavior.
Fields
mode enum (Mode)

Optional. Specifies the mode in which function calling should execute. If unspecified, the default value will be set to AUTO.
allowedFunctionNames[] string

Optional. A set of function names that, when provided, limits the functions the model will call.

This should only be set when the Mode is ANY. Function names should match [FunctionDeclaration.name]. With mode set to ANY, model will predict a function call from the set of function names provided.
JSON representation

{
  "mode": enum (Mode),
  "allowedFunctionNames": [
    string
  ]
}

Mode

Defines the execution behavior for function calling by defining the execution mode.
Enums
MODE_UNSPECIFIED 	Unspecified function calling mode. This value should not be used.
AUTO 	Default model behavior, model decides to predict either a function call or a natural language response.
ANY 	Model is constrained to always predicting a function call only. If "allowedFunctionNames" are set, the predicted function call will be limited to any one of "allowedFunctionNames", else the predicted function call will be any one of the provided "functionDeclarations".
NONE 	Model will not predict any function call. Model behavior is same as when not passing any function declarations.
VALIDATED 	Model decides to predict either a function call or a natural language response, but will validate function calls with constrained decoding.
UsageMetadata

Metadata on the usage of the cached content.
Fields
totalTokenCount integer

Total number of tokens that the cached content consumes.
JSON representation

{
  "totalTokenCount": integer
}

Was this helpful?

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-06 UTC.

    Terms
    Privacy

The new page has loaded.

Skip to main content

ai.google.dev uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more.
Google AI for Developers

Models
/
Sign in
Gemini API docs
API Reference
Cookbook
Community

Introducing updates to our 2.5 family of thinking models. Learn more

    Home
    Gemini API
    Models

Was this helpful?
Function calling with the Gemini API

Function calling lets you connect models to external tools and APIs. Instead of generating text responses, the model determines when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:

    Augment Knowledge: Access information from external sources like databases, APIs, and knowledge bases.
    Extend Capabilities: Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.
    Take Actions: Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices.

Python
JavaScript
REST

from google import genai
from google.genai import types

# Define the function declaration for the model
schedule_meeting_function = {
    "name": "schedule_meeting",
    "description": "Schedules a meeting with specified attendees at a given time and date.",
    "parameters": {
        "type": "object",
        "properties": {
            "attendees": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of people attending the meeting.",
            },
            "date": {
                "type": "string",
                "description": "Date of the meeting (e.g., '2024-07-29')",
            },
            "time": {
                "type": "string",
                "description": "Time of the meeting (e.g., '15:00')",
            },
            "topic": {
                "type": "string",
                "description": "The subject or topic of the meeting.",
            },
        },
        "required": ["attendees", "date", "time", "topic"],
    },
}

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[schedule_meeting_function])
config = types.GenerateContentConfig(tools=[tools])

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning.",
    config=config,
)

# Check for a function call
if response.candidates[0].content.parts[0].function_call:
    function_call = response.candidates[0].content.parts[0].function_call
    print(f"Function to call: {function_call.name}")
    print(f"Arguments: {function_call.args}")
    #  In a real app, you would call your function here:
    #  result = schedule_meeting(**function_call.args)
else:
    print("No function call found in the response.")
    print(response.text)

How function calling works

function calling overview

Function calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:

    Define Function Declaration: Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.
    Call LLM with function declarations: Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.
    Execute Function Code (Your Responsibility): The Model does not execute the function itself. It's your application's responsibility to process the response and check for Function Call, if
        Yes: Extract the name and args of the function and execute the corresponding function in your application.
        No: The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).
    Create User friendly response: If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.

This process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn (parallel function calling) and in sequence (compositional function calling).
Step 1: Define a function declaration

Define a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.
Python
JavaScript

# Define a function that the model can call to control smart lights
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}

Step 2: Call the model with function declarations

Once you have defined your function declarations, you can prompt the model to use them. It analyzes the prompt and function declarations and decides whether to respond directly or to call a function. If a function is called, the response object will contain a function call suggestion.
Python
JavaScript

from google.genai import types

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[set_light_values_declaration])
config = types.GenerateContentConfig(tools=[tools])

# Define user prompt
contents = [
    types.Content(
        role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
    )
]

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
    config=config,
)

print(response.candidates[0].content.parts[0].function_call)

The model then returns a functionCall object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.
Python
JavaScript

id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'

Step 3: Execute set_light_values function code

Extract the function call details from the model's response, parse the arguments , and execute the set_light_values function.
Python
JavaScript

# Extract tool call details, it may not be in the first part.
tool_call = response.candidates[0].content.parts[0].function_call

if tool_call.name == "set_light_values":
    result = set_light_values(**tool_call.args)
    print(f"Function execution result: {result}")

Step 4: Create user friendly response with function result and call the model again

Finally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.
Python
JavaScript

# Create a function response part
function_response_part = types.Part.from_function_response(
    name=tool_call.name,
    response={"result": result},
)

# Append function call and result of the function execution to contents
contents.append(response.candidates[0].content) # Append the content from the model's response.
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)

This completes the function calling flow. The model successfully used the set_light_values function to perform the request action of the user.
Function declarations

When you implement function calling in a prompt, you create a tools object, which contains one or more function declarations. You define functions using JSON, specifically with a select subset of the OpenAPI schema format. A single function declaration can include the following parameters:

    name (string): A unique name for the function (get_weather_forecast, send_email). Use descriptive names without spaces or special characters (use underscores or camelCase).
    description (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful ("Finds theaters based on location and optionally movie title which is currently playing in theaters.").
    parameters (object): Defines the input parameters the function expects.
        type (string): Specifies the overall data type, such as object.
        properties (object): Lists individual parameters, each with:
            type (string): The data type of the parameter, such as string, integer, boolean, array.
            description (string): A description of the parameter's purpose and format. Provide examples and constraints ("The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.").
            enum (array, optional): If the parameter values are from a fixed set, use "enum" to list the allowed values instead of just describing them in the description. This improves accuracy ("enum": ["daylight", "cool", "warm"]).
        required (array): An array of strings listing the parameter names that are mandatory for the function to operate.

Parallel function calling

In addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.
Python
JavaScript

power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}

Configure the function calling mode to allow using all of the specified tools. To learn more, you can read about configuring function calling.
Python
JavaScript

from google import genai
from google.genai import types

# Configure the client and tools
client = genai.Client()
house_tools = [
    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
]
config = types.GenerateContentConfig(
    tools=house_tools,
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
        disable=True
    ),
    # Force the model to call 'any' function, instead of chatting.
    tool_config=types.ToolConfig(
        function_calling_config=types.FunctionCallingConfig(mode='ANY')
    ),
)

chat = client.chats.create(model="gemini-2.5-flash", config=config)
response = chat.send_message("Turn this place into a party!")

# Print out each of the function calls requested from this single call
print("Example 1: Forced function calling")
for fn in response.function_calls:
    args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
    print(f"{fn.name}({args})")

Each of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested.

The Python SDK supports automatic function calling, which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.
Note: Automatic Function Calling is a Python SDK only feature at the moment.
Python

from google import genai
from google.genai import types

# Actual function implementations
def power_disco_ball_impl(power: bool) -> dict:
    """Powers the spinning disco ball.

    Args:
        power: Whether to turn the disco ball on or off.

    Returns:
        A status dictionary indicating the current state.
    """
    return {"status": f"Disco ball powered {'on' if power else 'off'}"}

def start_music_impl(energetic: bool, loud: bool) -> dict:
    """Play some music matching the specified parameters.

    Args:
        energetic: Whether the music is energetic or not.
        loud: Whether the music is loud or not.

    Returns:
        A dictionary containing the music settings.
    """
    music_type = "energetic" if energetic else "chill"
    volume = "loud" if loud else "quiet"
    return {"music_type": music_type, "volume": volume}

def dim_lights_impl(brightness: float) -> dict:
    """Dim the lights.

    Args:
        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

    Returns:
        A dictionary containing the new brightness setting.
    """
    return {"brightness": brightness}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Do everything you need to this place into party!",
    config=config,
)

print("\nExample 2: Automatic function calling")
print(response.text)
# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!

Compositional function calling

Compositional or sequential function calling allows Gemini to chain multiple function calls together to fulfill a complex request. For example, to answer "Get the temperature in my current location", the Gemini API might first invoke a get_current_location() function followed by a get_weather() function that takes the location as a parameter.

The following example demonstrates how to implement compositional function calling using the Python SDK and automatic function calling.
Python
JavaScript

This example uses the automatic function calling feature of the google-genai Python SDK. The SDK automatically converts the Python functions to the required schema, executes the function calls when requested by the model, and sends the results back to the model to complete the task.

import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20¬∞C in London, set the thermostat to 20¬∞C, otherwise set it to 18¬∞C.",
    config=config,
)

# Print the final, user-facing response
print(response.text)

Expected Output

When you run the code, you will see the SDK orchestrating the function calls. The model first calls get_weather_forecast, receives the temperature, and then calls set_thermostat_temperature with the correct value based on the logic in the prompt.

Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. I've set the thermostat to 20¬∞C.

Compositional function calling is a native Live API feature. This means Live API can handle the function calling similar to the Python SDK.
Python
JavaScript

# Light control schemas
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")

Function calling modes

The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the.function_calling_config.

    AUTO (Default): The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.
    ANY: The model is constrained to always predict a function call and guarantees function schema adherence. If allowed_function_names is not specified, the model can choose from any of the provided function declarations. If allowed_function_names is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).

    NONE: The model is prohibited from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.

Python
JavaScript

from google.genai import types

# Configure function calling mode
tool_config = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="ANY", allowed_function_names=["get_current_temperature"]
    )
)

# Create the generation config
config = types.GenerateContentConfig(
    tools=[tools],  # not defined here.
    tool_config=tool_config,
)

Automatic function calling (Python only)

When using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and the response cycle for you. The Python SDK then automatically:

    Detects function call responses from the model.
    Call the corresponding Python function in your code.
    Sends the function response back to the model.
    Returns the model's final text response.

To use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:
Python

from google import genai
from google.genai import types

# Define the function with type hints and docstring
def get_current_temperature(location: str) -> dict:
    """Gets the current temperature for a given location.

    Args:
        location: The city and state, e.g. San Francisco

    Returns:
        A dictionary containing the temperature and unit.
    """
    # ... (implementation) ...
    return {"temperature": 25, "unit": "Celsius"}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_current_temperature]
)  # Pass the function itself

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What's the temperature in Boston?",
    config=config,
)

print(response.text)  # The SDK handles the function call and returns the final text

You can disable automatic function calling with:
Python

config = types.GenerateContentConfig(
    tools=[get_current_temperature],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
)

Automatic function schema declaration

Automatic schema extraction from Python functions doesn't work in all cases. For example, it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:
Python

AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])

To see what the inferred schema looks like, you can convert it using from_callable:
Python

def multiply(a: float, b: float):
    """Returns a * b."""
    return a * b

fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

# to_json_dict() provides a clean JSON representation.
print(fn_decl.to_json_dict())

Multi-tool use: Combine native tools with function calling

You can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, Grounding with Google Search and code execution, in a request using the Live API.
Note: Multi-tool use is a-Live API only feature at the moment. The run() function declaration, which handles the asynchronous websocket setup, is omitted for brevity.
Python
JavaScript

# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")

Python developers can try this out in the Live API Tool Use notebook.
Model context protocol (MCP)

Model Context Protocol (MCP) is an open standard for connecting AI applications with external tools and data. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts.

The Gemini SDKs have built-in support for the MCP, reducing boilerplate code and offering automatic tool calling for MCP tools. When the model generates an MCP tool call, the Python and JavaScript client SDK can automatically execute the MCP tool and send the response back to the model in a subsequent request, continuing this loop until no more tool calls are made by the model.

Here, you can find an example of how to use a local MCP server with Gemini and mcp SDK.
Python
JavaScript

Make sure the latest version of the mcp SDK is installed on your platform of choice.

pip install mcp

Note: Python supports automatic tool calling by passing in the ClientSession into the tools parameters. If you want to disable it, you can provide automatic_function_calling with disabled True.

import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool
                    # Uncomment if you **don't** want the SDK to automatically call the tool
                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
                    #     disable=True
                    # ),
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())

Limitations with built-in MCP support

Built-in MCP support is a experimental feature in our SDKs and has the following limitations:

    Only tools are supported, not resources nor prompts
    It is available for the Python and JavaScript/TypeScript SDK.
    Breaking changes might occur in future releases.

Manual integration of MCP servers is always an option if these limit what you're building.
Supported models

This section lists models and their function calling capabilities. Experimental models are not included. You can find a comprehensive capabilities overview on the model overview page.
Model 	Function Calling 	Parallel Function Calling 	Compositional Function Calling
Gemini 2.5 Pro 	‚úîÔ∏è 	‚úîÔ∏è 	‚úîÔ∏è
Gemini 2.5 Flash 	‚úîÔ∏è 	‚úîÔ∏è 	‚úîÔ∏è
Gemini 2.5 Flash-Lite 	‚úîÔ∏è 	‚úîÔ∏è 	‚úîÔ∏è
Gemini 2.0 Flash 	‚úîÔ∏è 	‚úîÔ∏è 	‚úîÔ∏è
Gemini 2.0 Flash-Lite 	X 	X 	X
Best practices

    Function and Parameter Descriptions: Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.
    Naming: Use descriptive function names (without spaces, periods, or dashes).
    Strong Typing: Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.
    Tool Selection: While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.
    Prompt Engineering:
        Provide context: Tell the model its role (e.g., "You are a helpful weather assistant.").
        Give instructions: Specify how and when to use functions (e.g., "Don't guess dates; always use a future date for forecasts.").
        Encourage clarification: Instruct the model to ask clarifying questions if needed.
    Temperature: Use a low temperature (e.g., 0) for more deterministic and reliable function calls.
    Validation: If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.
    Error Handling: Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.
    Security: Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.
    Token Limits: Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.

Notes and limitations

    Only a subset of the OpenAPI schema is supported.
    Supported parameter types in Python are limited.
    Automatic function calling is a Python SDK feature only.

Was this helpful?

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

    Terms
    Privacy

The new page has loaded..

Skip to content
Navigation Menu
pydantic
pydantic-ai

Code
Issues 276
Pull requests 45
Actions
Projects
Security

    Insights

Schema error using MCP server with Gemini 2.5 #1250
Closed
#1342
Closed
Schema error using MCP server with Gemini 2.5
#1250
#1342
@rectalogic
Description
rectalogic
opened on Mar 26, 2025
Initial Checks

    I confirm that I'm using the latest version of Pydantic AI
    I confirm that I searched for my issue in https://github.com/pydantic/pydantic-ai/issues before opening this issue

Description

Gemini chokes on $schema in the JSON schema returned by MCP servers.

> GEMINI_API_KEY=XXX uv run --with pydantic-ai client.py
Secure MCP Filesystem Server running on stdio
Allowed directories: [ '/private/tmp' ]
Traceback (most recent call last):
  File "/private/tmp/client.py", line 15, in <module>
    asyncio.run(main())
  File "/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/private/tmp/client.py", line 11, in main
    result = await agent.run('Summarize the file client.py')
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/agent.py", line 327, in run
    async for _ in agent_run:
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/agent.py", line 1414, in __anext__
    next_node = await self._graph_run.__anext__()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_graph/graph.py", line 782, in __anext__
    return await self.next(self._next_node)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_graph/graph.py", line 760, in next
    self._next_node = await node.run(ctx)
                      ^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py", line 262, in run
    return await self._make_request(ctx)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/_agent_graph.py", line 314, in _make_request
    model_response, request_usage = await ctx.deps.model.request(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/models/gemini.py", line 131, in request
    async with self._make_request(
  File "/Users/aw/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aw/Library/Caches/uv/archive-v0/UT4lACLsamDruEsIvP3Xe/lib/python3.11/site-packages/pydantic_ai/models/gemini.py", line 227, in _make_request
    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=r.text)
pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gemini-2.5-pro-exp-03-25, body: {
  "error": {
    "code": 400,
    "message": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[0].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[1].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[2].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[3].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[4].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[5].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[6].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[7].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[8].parameters': Cannot find field.\nInvalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[9].parameters': Cannot find field.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.BadRequest",
        "fieldViolations": [
          {
            "field": "tools.function_declarations[0].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[0].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[1].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[1].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[2].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[2].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[3].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[3].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[4].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[4].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[5].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[5].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[6].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[6].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[7].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[7].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[8].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[8].parameters': Cannot find field."
          },
          {
            "field": "tools.function_declarations[9].parameters",
            "description": "Invalid JSON payload received. Unknown name \"$schema\" at 'tools.function_declarations[9].parameters': Cannot find field."
          }
        ]
      }
    ]
  }
}

Example Code

import asyncio
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio

server = MCPServerStdio('npx', ['-y', '@modelcontextprotocol/server-filesystem', '.'])
agent = Agent('gemini-2.5-pro-exp-03-25', mcp_servers=[server])


async def main():
    async with agent.run_mcp_servers():
        result = await agent.run('Summarize the file client.py')
    print(result.data)

if __name__ == "__main__":
    asyncio.run(main())

Python, Pydantic AI & LLM client version

Python 3.11.10
Pydantic 0.0.44
LLM gemini-2.5-pro-exp-03-25

Activity
rectalogic
added
need confirmation
on Mar 26, 2025
Kludex
Kludex commented on Mar 26, 2025
Kludex
on Mar 26, 2025
Member

That issue unfortunately is because the MCP server generates a more complete JSON schema, and Gemini doesn't support that.
rectalogic
rectalogic commented on Mar 26, 2025
rectalogic
on Mar 26, 2025
Author

Hmm, Gemini docs say they support "a select subset of the OpenAPI schema format". And I don't see $schema in that subset.
https://ai.google.dev/gemini-api/docs/function-calling#function_declarations

Is there any way in pydantic-ai to modify the schema returned from the MCP server to remove any $schema keys before it is sent to the LLM? e.g. post-process the ToolDefinition instances and modify their schemas?
rectalogic
rectalogic commented on Mar 26, 2025
rectalogic
on Mar 26, 2025
Author

OK, I was able to get it working with Gemini by subclassing the MCPServers to post-process the ToolDefinition:

import typing as t

from pydantic_ai.mcp import MCPServerHTTP, MCPServerStdio
from pydantic_ai.tools import ToolDefinition


class MCPServerMixin(MCPServer):
    @t.override
    async def list_tools(self) -> list[ToolDefinition]:
        return [self._modify_schema(tool) for tool in await super().list_tools()]

    def _modify_schema(self, tool: ToolDefinition) -> ToolDefinition:
        tool.parameters_json_schema.pop("$schema", None)
        return tool


class MCPServerStdioSchema(MCPServerMixin, MCPServerStdio):
    pass


class MCPServerHTTPSchema(MCPServerMixin, MCPServerHTTP):
    pass

rectalogic
closed this as completedon Mar 26, 2025
Kludex
reopened this on Mar 26, 2025
rectalogic
added a commit that references this issue on Mar 26, 2025

post-process ToolDefinition so it works with Gemini
e6f578f
TensorTemplar
TensorTemplar commented on Mar 27, 2025
TensorTemplar
on Mar 27, 2025

This affects all GoogleGLAProvider models in 0.46, not only Gemini 2.5
Kludex
mentioned this in 2 pull requests on Apr 2, 2025

Drop exclusiveMaximum/exclusiveMinimum from Gemini #1341

    Drop JSON schema url from schema on Gemini #1342

Kludex
Kludex commented on Apr 2, 2025
Kludex
on Apr 2, 2025
Member

    I've implemented the fix on 

    Drop JSON schema url from schema on Gemini #1342. üôè

Kludex
self-assigned this
on Apr 2, 2025
Kludex
added
bugSomething isn't working
and removed
need confirmation
on Apr 2, 2025
Kludex
closed this as completedin #1342on Apr 2, 2025
1 remaining item
drewano
Add a comment
new Comment
Markdown input: edit mode selected.
Remember, contributions to this repository should follow its contributing guidelines and security policy.
Metadata
Assignees

Labels
bugSomething isn't working
Type
No type
Projects
No projects
Milestone
No milestone

Relationships
None yet

Development

Notifications
You're not receiving notifications from this thread.
Participants
@rectalogic
@Kludex
@TensorTemplar
Issue actions

Footer
¬© 2025 GitHub, Inc.
Footer navigation

    Terms
    Privacy
    Security
    Status
    Docs
    Contact

Drop JSON schema url from schema on Gemini by Kludex ¬∑ Pull Request #1342 ¬∑ pydantic/pydantic-ai

Quand j'utilise le serveur MCP avec un mod√®le Gemini j'ai √ßa : The argument schema for tool mcp_data-inclusion_list_all_structures", is incompatible with gemini-2.5-flash-preview-05-20. Please fix the MCP server, disable the MCP server, or switch models.`
- **Stack Technique :** `FastMCP, Python`
- **Code du Projet :** Le contexte complet du code est fourni ci-dessous au format Repomix. Utilise-le pour comprendre la structure et les conventions actuelles.
- **Outils :** Cursor a acc√®s √† l'outil Context7 qui lui permet de chercher les documentations qu'il a besoins, n'h√©site pas √† lui mentionner cet outil.

---
