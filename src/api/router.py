"""
Router API pour l'agent IA d'inclusion sociale.

Ce module contient les routes et endpoints de l'API REST pour l'agent IA.
"""

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from pydantic_ai.messages import ModelMessage
from .dependencies import AgentDep

# Plus de variables globales ! ðŸŽ‰
api_router = APIRouter()

# Variable pour stocker l'instance de l'application (closure pattern pour Gradio)
_app_instance = None


def set_app_instance(app):
    """
    Sets the global application instance for integration with Gradio or other components.
    
    Parameters:
        app: The application instance to be stored globally.
    """
    global _app_instance
    _app_instance = app


def get_agent():
    """
    Retrieve the agent instance from the application's state.
    
    Returns:
        The agent object if available; otherwise, None.
    """
    if _app_instance is None:
        return None
    return getattr(_app_instance.state, "agent", None)


class ChatRequest(BaseModel):
    prompt: str
    history: list[ModelMessage] | None = None


@api_router.post("/chat/stream")
async def chat_stream(
    request: ChatRequest,
    agent: AgentDep,  # Injection de dÃ©pendance native !
) -> StreamingResponse:
    """
    Streams chat responses from the agent as server-sent events.
    
    Accepts a chat request and asynchronously streams the agent's response in text chunks, formatted for real-time consumption by clients.
    
    Returns:
        StreamingResponse: An asynchronous stream of text data with media type "text/plain".
    """

    async def generate():
        # Utiliser l'agent avec async with puis stream_text()
        """
        Asynchronously streams text chunks generated by the agent in response to the provided prompt.
        
        Yields:
            str: Each chunk of generated text, formatted as a server-sent event line.
        """
        async with agent.run_stream(request.prompt) as result:
            async for chunk in result.stream_text():
                yield f"data: {chunk}\n\n"

    return StreamingResponse(generate(), media_type="text/plain")
