"""
Interface de chat Gradio pour l'agent IA d'inclusion sociale.

Ce module contient l'interface utilisateur Gradio pour interagir avec l'agent IA.
"""

import logging
from typing import List, Dict, AsyncGenerator
import gradio as gr
from pydantic_ai import Agent
from pydantic_ai.messages import (
    FinalResultEvent,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    PartDeltaEvent,
    PartStartEvent,
    TextPartDelta,
    ToolCallPartDelta,
    ModelRequest,
    UserPromptPart,
    SystemPromptPart,
    TextPart,
    ModelResponse,
    ModelMessage,
)

# Imports locaux
from ..api.router import get_agent
from ..gradio_utils import (
    create_tool_call_message,
    create_tool_result_message,
    create_error_message,
    log_gradio_message,
)

# Configuration du logging
logger = logging.getLogger(__name__)


def _format_gradio_history(history: List[Dict[str, str]]) -> List[ModelMessage]:
    """
    Convertit l'historique Gradio au format pydantic-ai ModelMessage.

    Args:
        history: Historique des messages au format Gradio

    Returns:
        Liste des messages au format pydantic-ai
    """
    formatted_history: List[ModelMessage] = []

    for msg in history:
        if isinstance(msg, dict):
            # Nettoyer le message pour ne garder que les champs essentiels
            role = msg.get("role", "")
            content = msg.get("content", "")

            if role == "user" and content:
                # Cr√©er un ModelRequest avec UserPromptPart
                user_request = ModelRequest(parts=[UserPromptPart(content=content)])
                formatted_history.append(user_request)
            elif role == "assistant" and content:
                # Cr√©er un ModelResponse avec TextPart
                assistant_response = ModelResponse(parts=[TextPart(content=content)])
                formatted_history.append(assistant_response)
            elif role == "system" and content:
                # Cr√©er un ModelRequest avec SystemPromptPart
                system_request = ModelRequest(parts=[SystemPromptPart(content=content)])
                formatted_history.append(system_request)

    return formatted_history


async def _handle_agent_node(
    node, run_context, response_messages: List[gr.ChatMessage]
):
    """
    G√®re un n≈ìud de l'agent en d√©l√©guant √† la fonction appropri√©e selon le type de n≈ìud.

    Args:
        node: Le n≈ìud de l'agent √† traiter
        run_context: Le contexte d'ex√©cution de l'agent
        response_messages: Liste des messages de r√©ponse √† modifier

    Yields:
        List[gr.ChatMessage]: Messages mis √† jour pour le streaming
    """
    if Agent.is_user_prompt_node(node):
        # N≈ìud de prompt utilisateur
        logger.info(f"Traitement du message utilisateur: {node.user_prompt}")
        yield response_messages

    elif Agent.is_model_request_node(node):
        # N≈ìud de requ√™te mod√®le - d√©l√©guer au streaming de la r√©ponse
        async for _ in _stream_model_response(node, run_context, response_messages):
            yield response_messages

    elif Agent.is_call_tools_node(node):
        # N≈ìud d'appel d'outils - d√©l√©guer au streaming des appels d'outils
        async for _ in _stream_tool_calls(node, run_context, response_messages):
            yield response_messages

    elif Agent.is_end_node(node):
        # N≈ìud de fin - traitement termin√©
        logger.info("Traitement termin√© avec succ√®s")
        yield response_messages


async def _stream_model_response(
    node, run_context, response_messages: List[gr.ChatMessage]
):
    """
    G√®re le streaming de la r√©ponse du mod√®le.

    Args:
        node: Le n≈ìud de requ√™te mod√®le
        run_context: Le contexte d'ex√©cution de l'agent
        response_messages: Liste des messages de r√©ponse √† modifier

    Yields:
        List[gr.ChatMessage]: Messages mis √† jour pour le streaming
    """
    logger.info("Streaming de la requ√™te mod√®le...")

    # Ajouter un message assistant normal pour le streaming
    streaming_message = gr.ChatMessage(role="assistant", content="")
    response_messages.append(streaming_message)
    yield response_messages

    # Stream les tokens partiels
    async with node.stream(run_context) as request_stream:
        async for event in request_stream:
            if isinstance(event, PartStartEvent):
                logger.debug(f"D√©but de la partie {event.index}: {event.part}")
            elif isinstance(event, PartDeltaEvent):
                if isinstance(event.delta, TextPartDelta):
                    # Mettre √† jour le message avec le contenu stream√©
                    current_content = (
                        str(streaming_message.content)
                        if streaming_message.content
                        else ""
                    )
                    streaming_message.content = (
                        current_content + event.delta.content_delta
                    )
                    yield response_messages
                elif isinstance(event.delta, ToolCallPartDelta):
                    logger.debug(f"Appel d'outil en cours: {event.delta.args_delta}")
            elif isinstance(event, FinalResultEvent):
                logger.debug("Streaming de la r√©ponse termin√©")
                yield response_messages


async def _stream_tool_calls(
    node, run_context, response_messages: List[gr.ChatMessage]
):
    """
    G√®re le streaming des appels d'outils MCP.

    Args:
        node: Le n≈ìud d'appel d'outils
        run_context: Le contexte d'ex√©cution de l'agent
        response_messages: Liste des messages de r√©ponse √† modifier

    Yields:
        List[gr.ChatMessage]: Messages mis √† jour pour le streaming
    """
    logger.info("Traitement des appels d'outils...")

    async with node.stream(run_context) as handle_stream:
        async for event in handle_stream:
            if isinstance(event, FunctionToolCallEvent):
                # Afficher l'appel d'outil en utilisant l'utilitaire
                tool_call_message = create_tool_call_message(
                    event.part.tool_name,
                    event.part.args,
                    event.part.tool_call_id,
                )
                response_messages.append(tool_call_message)
                log_gradio_message(tool_call_message, "TOOL_CALL")
                yield response_messages

            elif isinstance(event, FunctionToolResultEvent):
                # Afficher le r√©sultat de l'outil en utilisant l'utilitaire
                result_message = create_tool_result_message(
                    tool_name="Outil MCP",  # Nom g√©n√©rique car pas disponible dans l'event
                    result=event.result.content,
                    call_id=event.tool_call_id,
                )
                response_messages.append(result_message)
                log_gradio_message(result_message, "TOOL_RESULT")
                yield response_messages


def create_complete_interface():
    """
    Cr√©e l'interface Gradio compl√®te avec streaming et affichage des appels aux outils MCP.
    """

    async def chat_stream(
        message: str, history: List[Dict[str, str]], request: gr.Request
    ) -> AsyncGenerator[List[gr.ChatMessage], None]:
        """
        Fonction de streaming pour l'interface de chat avec affichage des appels aux outils MCP.

        Args:
            message: Message de l'utilisateur
            history: Historique des messages
            request: Objet Request de Gradio (non utilis√© pour l'acc√®s √† l'agent)

        Yields:
            Listes de ChatMessage format√©es incluant les d√©tails des appels aux outils MCP
        """
        if not message or not message.strip():
            yield [
                gr.ChatMessage(
                    role="assistant", content="‚ö†Ô∏è Veuillez entrer un message valide."
                )
            ]
            return

        try:
            # Utilisation de l'agent r√©cup√©r√© depuis l'√©tat de l'application
            agent = get_agent()
            if agent is None:
                yield [
                    gr.ChatMessage(
                        role="assistant", content="‚ùå Erreur: Agent non initialis√©"
                    )
                ]
                return

            # Convertir l'historique Gradio au format pydantic-ai
            formatted_history = _format_gradio_history(history)

            # Initialiser la liste des messages de r√©ponse
            response_messages = []

            # Utiliser l'API avanc√©e d'it√©ration pour capturer les d√©tails des outils
            async with agent.iter(message, message_history=formatted_history) as run:
                async for node in run:
                    # G√©rer le n≈ìud avec la fonction d'aide appropri√©e et streamer les r√©sultats
                    async for messages in _handle_agent_node(
                        node, run.ctx, response_messages
                    ):
                        yield messages

                    # Si c'est un n≈ìud de fin, sortir de la boucle
                    if Agent.is_end_node(node):
                        break

        except Exception as e:
            logger.error(f"Erreur lors du streaming: {e}")
            error_message = create_error_message(str(e))
            log_gradio_message(error_message, "ERROR")
            yield [error_message]

    # Exemples de conversation
    examples = [
        "Bonjour ! Comment puis-je vous aider aujourd'hui ?",
        "Trouve des structures d'aide pr√®s de 75001 Paris",
        "Quels services d'insertion professionnelle √† Lyon ?",
        "Aide au logement d'urgence √† Marseille",
        "Services pour personnes handicap√©es √† Lille",
        "Comment obtenir une aide alimentaire ?",
        "Structures d'accueil pour familles monoparentales",
    ]

    # Cr√©er l'interface ChatInterface
    chat_interface = gr.ChatInterface(
        fn=chat_stream,
        type="messages",
        title="ü§ñ Agent IA d'Inclusion Sociale",
        description="Assistant intelligent sp√©cialis√© dans l'inclusion sociale en France - Affichage des appels aux outils MCP",
        examples=examples,
        cache_examples=False,
        chatbot=gr.Chatbot(
            label="Assistant IA",
            height=1100,
            show_copy_button=True,
            type="messages",
            avatar_images=(
                "https://em-content.zobj.net/source/twitter/376/bust-in-silhouette_1f464.png",
                "https://em-content.zobj.net/source/twitter/376/robot-face_1f916.png",
            ),
            placeholder="Bienvenue ! Posez votre question sur l'inclusion sociale...",
        ),
        textbox=gr.Textbox(
            placeholder="Ex: Aide au logement pr√®s de 75001 Paris",
            lines=1,
            max_lines=3,
            show_label=False,
        ),
    )

    return chat_interface
