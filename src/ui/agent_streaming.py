"""
Module de gestion du streaming de l'agent IA.

Ce module contient toute la logique de streaming et de traitement des n≈ìuds
de l'agent pydantic-ai, s√©par√©e de l'interface utilisateur Gradio.
"""

import logging
from typing import List, Dict, AsyncGenerator
import gradio as gr
from pydantic_ai import Agent
from pydantic_ai.messages import (
    FinalResultEvent,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    PartDeltaEvent,
    PartStartEvent,
    TextPartDelta,
    ToolCallPartDelta,
    ModelRequest,
    UserPromptPart,
    SystemPromptPart,
    TextPart,
    ModelResponse,
    ModelMessage,
)

# Imports locaux
from .gradio_utils import (
    create_tool_call_message,
    create_tool_result_message,
    create_error_message,
    log_gradio_message,
)

# Configuration du logging
logger = logging.getLogger("datainclusion.agent.streaming")


def format_gradio_history(history: List[Dict[str, str]]) -> List[ModelMessage]:
    """
    Convertit l'historique Gradio au format pydantic-ai ModelMessage.

    Filtre les messages d'outils pour ne pas polluer l'historique de l'agent.
    Ne garde que les messages utilisateur et assistant principaux.

    Args:
        history: Historique des messages au format Gradio

    Returns:
        Liste des messages au format pydantic-ai
    """
    formatted_history: List[ModelMessage] = []

    for msg in history:
        if isinstance(msg, dict):
            # Nettoyer le message pour ne garder que les champs essentiels
            role = msg.get("role", "")
            content = msg.get("content", "")

            # Filtrer les messages d'outils en v√©rifiant les m√©tadonn√©es
            metadata = msg.get("metadata", {})
            if metadata and isinstance(metadata, dict):
                title = metadata.get("title", "")
                # Ignorer les messages d'outils (ceux qui commencent par üõ†Ô∏è ou ‚úÖ/‚ùå)
                if (
                    title.startswith("üõ†Ô∏è")
                    or title.startswith("‚úÖ")
                    or title.startswith("‚ùå")
                ):
                    continue

            if role == "user" and content:
                # Cr√©er un ModelRequest avec UserPromptPart
                user_request = ModelRequest(parts=[UserPromptPart(content=content)])
                formatted_history.append(user_request)
            elif role == "assistant" and content:
                # Cr√©er un ModelResponse avec TextPart
                assistant_response = ModelResponse(parts=[TextPart(content=content)])
                formatted_history.append(assistant_response)
            elif role == "system" and content:
                # Cr√©er un ModelRequest avec SystemPromptPart
                system_request = ModelRequest(parts=[SystemPromptPart(content=content)])
                formatted_history.append(system_request)

    return formatted_history


async def handle_agent_node(
    node, run_context, response_messages: List[gr.ChatMessage]
) -> AsyncGenerator[List[gr.ChatMessage], None]:
    """
    G√®re un n≈ìud de l'agent en d√©l√©guant √† la fonction appropri√©e selon le type de n≈ìud.

    Args:
        node: Le n≈ìud de l'agent √† traiter
        run_context: Le contexte d'ex√©cution de l'agent
        response_messages: Liste des messages de r√©ponse √† modifier

    Yields:
        List[gr.ChatMessage]: Messages mis √† jour pour le streaming
    """
    if Agent.is_user_prompt_node(node):
        # N≈ìud de prompt utilisateur
        logger.info(f"Traitement du message utilisateur: {node.user_prompt}")
        yield response_messages

    elif Agent.is_model_request_node(node):
        # N≈ìud de requ√™te mod√®le - d√©l√©guer au streaming de la r√©ponse
        async for messages in stream_model_response(
            node, run_context, response_messages
        ):
            yield messages

    elif Agent.is_call_tools_node(node):
        # N≈ìud d'appel d'outils - d√©l√©guer au streaming des appels d'outils
        async for messages in stream_tool_calls(node, run_context, response_messages):
            yield messages

    elif Agent.is_end_node(node):
        # N≈ìud de fin - traitement termin√©
        logger.info("Traitement termin√© avec succ√®s")
        yield response_messages


async def stream_model_response(
    node, run_context, response_messages: List[gr.ChatMessage]
) -> AsyncGenerator[List[gr.ChatMessage], None]:
    """
    G√®re le streaming de la r√©ponse du mod√®le.

    Args:
        node: Le n≈ìud de requ√™te mod√®le
        run_context: Le contexte d'ex√©cution de l'agent
        response_messages: Liste des messages de r√©ponse √† modifier

    Yields:
        List[gr.ChatMessage]: Messages mis √† jour pour le streaming
    """
    logger.info("Streaming de la requ√™te mod√®le...")

    # Ajouter un message assistant vide pour le streaming
    streaming_message = gr.ChatMessage(role="assistant", content="")
    response_messages.append(streaming_message)
    yield response_messages

    # Stream les tokens partiels
    async with node.stream(run_context) as request_stream:
        async for event in request_stream:
            if isinstance(event, PartStartEvent):
                logger.debug(f"D√©but de la partie {event.index}: {event.part}")
                # Traiter le contenu initial si c'est un TextPart
                if isinstance(event.part, TextPart) and event.part.content:
                    # Initialiser le contenu du message avec le contenu initial
                    streaming_message.content = event.part.content
                    yield response_messages

            elif isinstance(event, PartDeltaEvent):
                if isinstance(event.delta, TextPartDelta):
                    # Mettre √† jour le message avec le contenu stream√©
                    current_content = (
                        str(streaming_message.content)
                        if streaming_message.content
                        else ""
                    )
                    streaming_message.content = (
                        current_content + event.delta.content_delta
                    )
                    yield response_messages
                elif isinstance(event.delta, ToolCallPartDelta):
                    logger.debug(f"Appel d'outil en cours: {event.delta.args_delta}")

            elif isinstance(event, FinalResultEvent):
                logger.debug("Streaming de la r√©ponse termin√©")
                yield response_messages


async def stream_tool_calls(
    node, run_context, response_messages: List[gr.ChatMessage]
) -> AsyncGenerator[List[gr.ChatMessage], None]:
    """
    G√®re le streaming des appels d'outils MCP.

    Args:
        node: Le n≈ìud d'appel d'outils
        run_context: Le contexte d'ex√©cution de l'agent
        response_messages: Liste des messages de r√©ponse √† modifier

    Yields:
        List[gr.ChatMessage]: Messages mis √† jour pour le streaming
    """
    logger.info("Traitement des appels d'outils...")

    # Mapper les tool_call_id vers les noms d'outils
    tool_call_map = {}

    async with node.stream(run_context) as handle_stream:
        async for event in handle_stream:
            if isinstance(event, FunctionToolCallEvent):
                # Stocker le nom de l'outil pour usage ult√©rieur
                tool_call_map[event.part.tool_call_id] = event.part.tool_name

                # Afficher l'appel d'outil en utilisant l'utilitaire
                tool_call_message = create_tool_call_message(
                    event.part.tool_name,
                    event.part.args,
                    event.part.tool_call_id,
                )
                response_messages.append(tool_call_message)
                log_gradio_message(tool_call_message, "TOOL_CALL")
                yield response_messages

            elif isinstance(event, FunctionToolResultEvent):
                # R√©cup√©rer le nom de l'outil depuis le mapping
                tool_name = tool_call_map.get(event.tool_call_id, "Outil MCP")

                # Afficher le r√©sultat de l'outil en utilisant l'utilitaire
                result_message = create_tool_result_message(
                    tool_name=tool_name,
                    result=event.result.content,
                    call_id=event.tool_call_id,
                )
                response_messages.append(result_message)
                log_gradio_message(result_message, "TOOL_RESULT")
                yield response_messages


async def process_agent_stream(
    agent, message: str, history: List[Dict[str, str]]
) -> AsyncGenerator[List[gr.ChatMessage], None]:
    """
    Traite le streaming complet d'un agent avec gestion des erreurs.

    Args:
        agent: Instance de l'agent pydantic-ai
        message: Message de l'utilisateur
        history: Historique des messages

    Yields:
        List[gr.ChatMessage]: Messages mis √† jour pour le streaming
    """
    try:
        # Convertir l'historique Gradio au format pydantic-ai
        formatted_history = format_gradio_history(history)

        # Initialiser la liste des messages de r√©ponse
        response_messages = []

        # Utiliser l'API avanc√©e d'it√©ration pour capturer les d√©tails des outils
        async with agent.iter(message, message_history=formatted_history) as run:
            async for node in run:
                # G√©rer le n≈ìud avec la fonction d'aide appropri√©e et streamer les r√©sultats
                async for messages in handle_agent_node(
                    node, run.ctx, response_messages
                ):
                    yield messages

                # Si c'est un n≈ìud de fin, sortir de la boucle
                if Agent.is_end_node(node):
                    break

    except Exception as e:
        logger.error(f"Erreur lors du streaming: {e}")
        error_message = create_error_message(str(e))
        log_gradio_message(error_message, "ERROR")
        yield [error_message]
