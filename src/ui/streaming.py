"""
Module de streaming moderne pour Chainlit + Pydantic-AI.

Cette implÃ©mentation suit les meilleures pratiques officielles de Pydantic-AI et Chainlit
pour un streaming en temps rÃ©el avec affichage transparent des outils MCP.

Architecture:
- Utilise `agent.iter()` pour parcourir le graphe d'exÃ©cution nÅ“ud par nÅ“ud
- Traite les diffÃ©rents types de nÅ“uds (UserPrompt, ModelRequest, CallTools, End)
- Utilise les Ã©vÃ©nements de streaming de Pydantic-AI pour le temps rÃ©el
- IntÃ¨gre parfaitement avec Chainlit (cl.Message.stream_token, cl.Step)
"""

import logging
from typing import List, Optional, Dict

import chainlit as cl
from pydantic_ai import Agent
from pydantic_ai.messages import (
    ModelMessage,
    PartStartEvent,
    PartDeltaEvent,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    TextPart,
    TextPartDelta,
    ToolCallPartDelta,
)

# Configuration du logging
logger = logging.getLogger("datainclusion.streaming")

# Constante pour limiter l'historique
MAX_HISTORY_LENGTH = 50


def trim_message_history(messages: List[ModelMessage]) -> List[ModelMessage]:
    """
    Limite l'historique des messages pour Ã©viter les problÃ¨mes de mÃ©moire.

    Args:
        messages: Liste des messages

    Returns:
        Liste tronquÃ©e des messages les plus rÃ©cents
    """
    if len(messages) <= MAX_HISTORY_LENGTH:
        return messages
    return messages[-MAX_HISTORY_LENGTH:]


async def _handle_user_prompt_node(node) -> None:
    """GÃ¨re le nÅ“ud UserPromptNode."""
    logger.debug("ğŸ“¨ UserPromptNode: %s", node.user_prompt)
    # Pas d'affichage spÃ©cial nÃ©cessaire, le message utilisateur est dÃ©jÃ  affichÃ©


async def _handle_model_request_node(
    node, agent_run, response_message: Optional[cl.Message]
) -> cl.Message:
    """GÃ¨re le nÅ“ud ModelRequestNode avec streaming des tokens."""
    logger.debug("ğŸ§  ModelRequestNode: Streaming de la rÃ©ponse LLM...")
    # Streamer la rÃ©ponse du modÃ¨le
    async with node.stream(agent_run.ctx) as request_stream:
        async for event in request_stream:
            response_message = await _handle_model_event(event, response_message)
    return response_message


async def _handle_model_event(
    event, response_message: Optional[cl.Message]
) -> cl.Message:
    """GÃ¨re un Ã©vÃ©nement de streaming du modÃ¨le."""
    # DÃ©but d'une nouvelle partie de rÃ©ponse
    if isinstance(event, PartStartEvent):
        logger.debug(
            "ğŸ”„ DÃ©but partie %s: %s",
            event.index,
            type(event.part).__name__,
        )
        # Si c'est une partie texte, crÃ©er le message de rÃ©ponse
        if isinstance(event.part, TextPart) and event.part.content:
            if response_message is None:
                response_message = cl.Message(content="")
                await response_message.send()
            # Streamer le contenu initial
            if response_message:
                await response_message.stream_token(event.part.content)

    # Delta de texte - streaming en temps rÃ©el
    elif isinstance(event, PartDeltaEvent):
        if isinstance(event.delta, TextPartDelta):
            # CrÃ©er le message de rÃ©ponse maintenant, quand on a du contenu
            if response_message is None and event.delta.content_delta:
                response_message = cl.Message(content="")
                await response_message.send()

            # Streamer chaque token vers Chainlit
            if event.delta.content_delta and response_message:
                await response_message.stream_token(event.delta.content_delta)

        elif isinstance(event.delta, ToolCallPartDelta):
            # Les appels d'outils sont traitÃ©s dans CallToolsNode
            logger.debug("ğŸ”§ Tool call delta: %s", event.delta.args_delta)

    return response_message


async def _handle_call_tools_node(
    node, agent_run, active_tool_steps: Dict[str, cl.Step], tool_call_counter: int
) -> int:
    """GÃ¨re le nÅ“ud CallToolsNode avec affichage des outils."""
    logger.debug("ğŸ› ï¸ CallToolsNode: Traitement des outils MCP...")

    # Streamer les Ã©vÃ©nements des outils
    async with node.stream(agent_run.ctx) as tools_stream:
        async for event in tools_stream:
            if isinstance(event, FunctionToolCallEvent):
                tool_call_counter += 1
                tool_call_counter = await _handle_tool_call_event(
                    event, active_tool_steps, tool_call_counter
                )
            elif isinstance(event, FunctionToolResultEvent):
                await _handle_tool_result_event(event, active_tool_steps)

    return tool_call_counter


async def _handle_tool_call_event(
    event, active_tool_steps: Dict[str, cl.Step], tool_call_counter: int
) -> int:
    """GÃ¨re un Ã©vÃ©nement d'appel d'outil."""
    tool_name = event.part.tool_name
    tool_args = event.part.args
    tool_call_id = event.part.tool_call_id

    logger.info("ğŸ”§ Appel outil: %s", tool_name)

    # CrÃ©er un Step pour l'appel d'outil
    step = cl.Step(
        name=f"ğŸ”§ {tool_name}",
        type="tool",
        show_input="json" if tool_args else False,
        language="json",
    )

    # Stocker le Step pour rÃ©cupÃ©rer le rÃ©sultat plus tard
    active_tool_steps[tool_call_id] = step

    # Configurer l'input du step
    with step:
        if tool_args:
            step.input = tool_args

    return tool_call_counter


async def _handle_tool_result_event(
    event, active_tool_steps: Dict[str, cl.Step]
) -> None:
    """GÃ¨re un Ã©vÃ©nement de rÃ©sultat d'outil."""
    tool_call_id = event.tool_call_id
    result_content = event.result.content

    # RÃ©cupÃ©rer le Step correspondant
    if tool_call_id in active_tool_steps:
        step = active_tool_steps[tool_call_id]

        # Configurer l'output du step
        step.output = str(result_content)[:1000]  # Limiter pour l'affichage

        # Finaliser le step
        await step.__aexit__(None, None, None)

        # Nettoyer le dictionnaire
        del active_tool_steps[tool_call_id]

        logger.info(
            "âœ… RÃ©sultat outil reÃ§u: %s chars",
            len(str(result_content)),
        )


async def _handle_end_node(node, response_message: Optional[cl.Message]) -> cl.Message:
    """GÃ¨re le nÅ“ud EndNode."""
    logger.info("ğŸ EndNode: ExÃ©cution terminÃ©e")
    final_output = str(node.data.output)

    # Si pas encore de message de rÃ©ponse crÃ©Ã©, le crÃ©er maintenant
    if response_message is None:
        response_message = cl.Message(content=final_output)
        await response_message.send()
    # Si pas encore de contenu dans le message (cas rare), l'ajouter
    elif not response_message.content.strip():
        await response_message.stream_token(final_output)

    return response_message


async def _cleanup_on_error(active_tool_steps: Dict[str, cl.Step]) -> None:
    """Nettoie les steps en cas d'erreur."""
    for step in active_tool_steps.values():
        try:
            await step.__aexit__(None, None, None)
        except Exception:
            pass


async def process_agent_with_perfect_streaming(
    agent: Agent, message: str, message_history: Optional[List[ModelMessage]] = None
) -> List[ModelMessage]:
    """
    Traite un agent avec streaming parfait selon les docs Pydantic-AI et Chainlit.

    Cette implÃ©mentation utilise:
    - agent.iter() pour parcourir le graphe d'exÃ©cution nÅ“ud par nÅ“ud
    - Events streaming de Pydantic-AI pour le temps rÃ©el
    - cl.Step pour afficher les outils MCP de maniÃ¨re transparente
    - cl.Message.stream_token() pour le streaming des tokens

    Args:
        agent: Instance de l'agent Pydantic-AI
        message: Message de l'utilisateur
        message_history: Historique des messages au format Pydantic-AI

    Returns:
        Liste mise Ã  jour des messages pour l'historique
    """
    try:
        logger.info("ğŸš€ DÃ©marrage du streaming parfait pour: %s...", message[:50])

        # Ne pas crÃ©er le message de rÃ©ponse tout de suite
        # Il sera crÃ©Ã© seulement quand on commence Ã  streamer du texte
        response_message: Optional[cl.Message] = None

        # Dictionnaire pour tracker les outils en cours
        active_tool_steps: Dict[str, cl.Step] = {}
        tool_call_counter = 0

        # Utiliser agent.iter() comme recommandÃ© dans la documentation
        async with agent.iter(
            message, message_history=message_history or []
        ) as agent_run:
            # Parcourir chaque nÅ“ud du graphe d'exÃ©cution
            async for node in agent_run:
                # 1. UserPromptNode - Message utilisateur reÃ§u
                if Agent.is_user_prompt_node(node):
                    await _handle_user_prompt_node(node)

                # 2. ModelRequestNode - RequÃªte vers le LLM avec streaming des tokens
                elif Agent.is_model_request_node(node):
                    response_message = await _handle_model_request_node(
                        node, agent_run, response_message
                    )

                # 3. CallToolsNode - Appels d'outils MCP avec affichage transparent
                elif Agent.is_call_tools_node(node):
                    tool_call_counter = await _handle_call_tools_node(
                        node, agent_run, active_tool_steps, tool_call_counter
                    )

                # 4. EndNode - Fin de l'exÃ©cution
                elif Agent.is_end_node(node):
                    response_message = await _handle_end_node(node, response_message)

        # Finaliser le message de rÃ©ponse s'il existe
        if response_message is not None:
            await response_message.update()

        # RÃ©cupÃ©rer l'historique complet (s'assurer que le result n'est pas None)
        if agent_run.result is not None:
            all_messages = agent_run.result.all_messages()
            trimmed_messages = trim_message_history(all_messages)
        else:
            logger.warning("agent_run.result est None, retour de l'historique original")
            trimmed_messages = message_history or []

        logger.info(
            "âœ… Streaming terminÃ© - Historique: %s messages", len(trimmed_messages)
        )
        return trimmed_messages

    except Exception as e:
        logger.error("âŒ Erreur dans le streaming parfait: %s", e, exc_info=True)

        # Nettoyage des steps ouverts en cas d'erreur
        await _cleanup_on_error(active_tool_steps)

        # Message d'erreur Ã  l'utilisateur
        error_msg = cl.Message(
            content=f"âŒ **Erreur lors du traitement:**\n\n{str(e)}\n\n"
            "Veuillez rÃ©essayer ou reformuler votre question."
        )
        await error_msg.send()

        return message_history or []


async def process_agent_fallback_simple(
    agent: Agent, message: str, message_history: Optional[List[ModelMessage]] = None
) -> List[ModelMessage]:
    """
    Version de fallback simple sans streaming pour la robustesse.

    Args:
        agent: Instance de l'agent Pydantic-AI
        message: Message de l'utilisateur
        message_history: Historique des messages

    Returns:
        Liste mise Ã  jour des messages
    """
    try:
        logger.info("ğŸ”„ Utilisation du fallback simple...")

        # ExÃ©cution simple sans streaming
        result = await agent.run(message, message_history=message_history or [])

        if result is None:
            raise ValueError("L'agent a retournÃ© un rÃ©sultat null")

        # Afficher la rÃ©ponse
        response_content = str(result.output)
        response_message = cl.Message(content=response_content)
        await response_message.send()

        # Retourner l'historique
        if hasattr(result, "all_messages"):
            return result.all_messages()
        logger.warning(
            "Result n'a pas d'attribut all_messages, retour historique original"
        )
        return message_history or []

    except Exception as e:
        logger.error("âŒ Erreur mÃªme en fallback: %s", e)

        error_message = cl.Message(
            content=f"âŒ **Erreur systÃ¨me:**\n\n{str(e)}\n\n"
            "Veuillez contacter l'administrateur si le problÃ¨me persiste."
        )
        await error_message.send()

        return message_history or []


async def process_agent_modern_with_history(
    agent: Agent, message: str, message_history: Optional[List[ModelMessage]] = None
) -> List[ModelMessage]:
    """
    Point d'entrÃ©e principal pour le traitement moderne avec streaming parfait.

    Cette fonction est le point d'entrÃ©e recommandÃ© qui utilise la meilleure
    implÃ©mentation disponible avec fallback automatique en cas d'erreur.

    Args:
        agent: Instance de l'agent Pydantic-AI
        message: Message de l'utilisateur
        message_history: Historique des messages au format Pydantic-AI

    Returns:
        Liste mise Ã  jour des messages pour l'historique de session
    """
    logger.info("ğŸ¯ Traitement moderne avec streaming parfait")

    try:
        # Tenter le streaming parfait
        return await process_agent_with_perfect_streaming(
            agent, message, message_history
        )

    except Exception as e:
        logger.warning("âš ï¸ Ã‰chec du streaming parfait, fallback: %s", e)

        # En cas d'Ã©chec, utiliser le fallback simple
        return await process_agent_fallback_simple(agent, message, message_history)
